\documentclass[a4paper]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}                   		
\usepackage{graphicx,subcaption}					
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage[square,sort,comma]{natbib}
\usepackage{lineno}
\usepackage{setspace}
\usepackage{booktabs,multirow}
\usepackage{authblk}

\renewcommand{\arraystretch}{1.3}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}


\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newcommand{\E}{\mathrm{E}}
\newcommand{\D}{\mathcal{D}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\R}{\texttt{R}}
\newcommand{\asreml}{\texttt{ASReml-R}}
\newcommand{\brms}{\texttt{brms}}
\newcommand{\rstan}{\texttt{rstan}}
\newcommand{\elpd}{elpd\textsubscript{loo}}
\newcommand{\psis}{elpd\textsubscript{psis-loo}}
\newcommand{\ploo}{p\textsubscript{loo}}

\newcommand{\BigO}[1]{{\rm O}\left(#1\right)}
\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\iid}{\textrm{i.i.d.\ }}

\title{Bayesian Inference of Spatially Correlated Random Parameters for On-farm Experiment}
%\author{Jerome}
\author[1]{Zhanglong Cao}
\author[1]{Suman Rakshit}
\author[2]{XXX}

\affil[1]{SAGI}
\affil[2]{\LaTeX\ University}

\date{}							% Activate to display a given date or no date

\linenumbers
%\doublespacing
\onehalfspacing

\begin{document}
\maketitle


\begin{abstract}
Accommodating spatial variation is common in analysing field trials, and has become a challenge in large on-farm experiments. Simple linear mixed models and Gaussian distribution are incapable of dealing with complex data sets and results in misspecification. This paper describes a Bayesian framework to analyse the model with spatially correlated random parameters for large on-farm experiments. With advanced model diagnostic tools, we found that the model with Gaussian assumption is misspecified. Therefore, we use Student-$t$ distribution with which the model is augmented. The open-source \R-code that implements our proposed method for analysing on-farm data is provided for further use. We also discuss the difference of the Bayesian approach and GWR, and compare the results from these two approaches. 
\end{abstract}


%\subsection*{Highlights}
%
%\begin{itemize}
%	\item[1] Application of Bayesian technique for on-farm experiments.
%	\item[2] Application with \R-packages \rstan.
%\end{itemize}
%
%\subsection*{Concerns}
%\begin{itemize}
%	\item[1] Is the manuscript well organised?
%	\item[2] Are statistical models well explained?
%	\item[3] Is the analysis adequate? 
%\end{itemize}


\section{Introduction}


Traditional agricultural experiments are conducted as replicated small-plots at one or multiple designated locations for a period of time. However, farmers prefer to conduct experiments to test and verify systems on their own farms even though the data are correlated as well as heterogeneous across space and the inference would be considered invalid under classical statistics \citep{Griffin2008Spatial}. Therefore, the use of on-farm (OF) trials or on-farm experiments (OFE) has been developed in recent years \citep{Troyer2009Heterosis, Yan2002Onfarm}. 


A problem of OFE data is the inherent spatial variation, which occurs when the values of variables sampled at nearby locations are not independent from each other and may seriously bias treatment estimates and inflate standard errors \citep{Tobler1970AComputer}. Spatial variation arises from small changes, such as fertility, soil moisture, light et al, or is aligned with the rows and columns of a field trial. It could also arise from experimental procedures or management practices that have a recurrent pattern, known as extraneous variation \citep{Gilmour1997Accounting, Hinkelmann2012Design}. Appropriate statistical approaches for analysing OFE data are in high demand. 
%For a review of design and analysis of OFE, see \cite{Piepho2011Onestimation} and \cite{ Schmidt2018More}.


A few researches have been conducted on this topic and discussed the accommodation of spatial variation, such as \citep{Selle2019Flexible, Montesinos-Lopez2018Multivariate}. These approaches are good for variety selection, where a variety replicates two or more times and the results are averaged across the field. However, their approaches are incapable of addressing the objective of precision farming: how to maximise the output and/or profit by developing management practices that lead to optimal utilisation of resources across variable environments \citep{Cook2013Onfarm}.


\cite{Rakshit2020Novel} proposed a geographically weighted regression (GWR) approach which is adapted to obtain spatially-varying estimates of treatment effects for OFE. They addressed the problem with the proposed model and found optimal Nitrogen rate for each single plot on the field. A problem with their approach is that the choice of bandwidth of kernel functions affects the results and may cause biases. 



In this paper, we use a Bayesian approach to fit the geo-referenced model for OFE data and to find the optimal treatment for each plot. Instead of imposing spatially correlated residuals, we found that the model with spatially correlated random parameters is good enough for capturing the spatial variation. Besides, 
%the Bayesian approach partitions complicated models into simple components, each of which may be formulated analytically . 
the adoption of the Bayesian approaches simplifies the interpretation of the results and augments the inference \citep{Che2010Bayesian}. Compared with REML (restricted maximum likelihood), \cite{Omer2017Comparing, Guan2017Estimation} demonstrate the advantages in terms of variation control and powerful inference.



Markov chain Monte Carlo (MCMC) methods have become important in Bayesian computation, and they allow inferences to be drawn from complex posterior distribution where analytical or numerical integration techniques cannot be applied \citep{Sorensen2007Likelihood}. By using MCMC, complex formulations can be analysed with comparative ease \citep{Besag1999Bayesian}. Apart from the classical Metropolis-Hastings (MH) algorithm \citep{Metropolis1953Equation, Hastings1970Monte}, the Gibbs sampling \citep{Geman1984Stochastic, Gelfand1990Samplingbased} is a special case of MH and is a method for sampling from distributions over at least two dimensions. See \citep{Theobald2002Bayesian, Che2010Bayesian, Omer2017Comparing, Chen2019Simple, Montesinos-Lopez2018Multivariate} for examples. However, the main problem of Gibbs sampling is that it update a single parameter at one time based on current status and end up with random walk behaviour, higher auto-correlation and slow mixing. So it has insufficient capability in dealing with correlated parameters for high-dimensional models \citep{Brooks2011Handbook}, and may require an unacceptably long time to converge to the target distribution \citep{Hoffman2014NoUturn}


Alternatively, the Hamiltonian Monte Carlo (HMC), originally called hybrid Monte Carlo \citep{Duane1987Hybrid}, is a MCMC algorithm that makes use of gradient information to reduce random walk behaviour. HMC uses ``momentum'', a definition from physics, variables that accelerate each iteration within a parameter space to allow faster mixing and convergence \citep{Ngombe2020Are, Brooks2011Handbook}. A problem with HMC is that it is highly sensitive to two user-specified parameters: a step size $\epsilon$ and a desired number of steps $L$. A poor choice decreases the efficiency of HMC dramatically. Therefore, the No-U-Turn Sampler (NUTS) was developed as an extension to HMC by \cite{Hoffman2014NoUturn}. The advantage of NUTS is that it determines the step size by adapting it during the warm-up (burn-in) phase to a target acceptance rate, and then used it for all sampling iterations \citep{Monnahan2017Faster}. It is a promising sampling method because of its good sampling qualities: large effective sample sizes, low auto-correlations, and low skewness of marginal posterior distributions \citep{Nishio2019Performance}. It is embedded with the \R\ package \rstan. The advantage of using \rstan\, rather than other ``black box'' packages, is that it has flexibility for researchers to implement complex and \textit{ad hoc} models for particular problems. Besides, the NUTS does not require conjugate priors. As a summary, the advantages of NUTS are faster convergence for multi-parameters and has higher flexibility for self-defined models by researchers.


We estimated the Bayesian inference and posterior distribution of the model that incorporates spatially correlated random parameters of OFE data in \rstan. Usually, that is the end of the story in most circumstances. However, with advanced model diagnostic tools, such as probability integral transformation (PIT) checks, and evaluation methods, such as Bayesian leave-one-out (LOO) cross validation (CV) for model comparison, we found that the model with Gaussian distribution is misspecified, and the influential points/outliers are inevitable. Therefore, we suggest to use Student-$t$ distribution and the results are improved. 


The objective of this paper is to demonstrate the capability of the Bayesian approach in analysing spatially correlated data for OFE and the power of the approach for precision agriculture. We suggest researchers run comprehensive model diagnosis and ``think out of the box'' that sometimes Gaussian distribution does not work well. Our \rstan\ script is open to public and can be applied to similar data by other users. The paper is organised as follow: in Section \ref{sec:model} we propose a generic spatial model for OFE data; in Section \ref{sec:bayes} we discuss the prior and posterior distribution for the model, and explained the mechanism of NUTS sampler; in Section \ref{sec:mcmcchain} we discuss the post-sampling model checking and diagnostic process; finally, in Section \ref{sec:analysis} we apply the proposed model and Bayesian framework to Las Rosas corn yield data set, and compare it with GWR.  



\section{Statistical models}\label{sec:model}


\cite{Piepho2011Statistical} summarise three general options regarding the range of spatial correlation for modelling geo-referenced measurements: spatial correlation within but not between plots \citep{Ritter2008AnOnfarm}, spatial correlation across a whole block but not between blocks \citep{Piepho2008Nearest} and spatial correlation across the whole field \citep{Hong2005Spatial, Hurley2004Estimating}. For OFE data, we focus on the third option. 


\subsection{Generic statistical model}

Let $y$ denote the response variable, it is assumed that $y$ varies on an experimental field and spatially correlated. The distribution is assumed a multivariate distribution, denoted as $\D$, with the predictor $\eta$ and covariance matrix $\Sigma$. The distribution $\D$ is usually a Gaussian or a student-$t$ distribution. We write 
\begin{eqnarray}\label{eq:ynorm}
y \sim M\D(\eta(\phi),\Sigma(\sigma)),
\end{eqnarray}
where $\theta=\lbrace \phi,\sigma\rbrace$ is a set of parameters. The predictor $\eta$ can be either linear or non-linear. The structure of the variance-covariance matrix $\Sigma(\sigma)$ may be written as
\begin{eqnarray}\label{eq:sigma}
\Sigma(\sigma) = V_s+\sigma^2I,
\end{eqnarray}
where $V_s$ is some spatial covariance structure and $\sigma^2$ is a nugget variance. Alternatively, \cite{Gilmour1997Accounting} suggest to use a separable first-order autoregressive process, $AR1\times AR1$, to commence the spatial modelling process in such way that 
\begin{eqnarray}\label{eq:ar1}
\Sigma(\sigma) = \sigma_s^2V_s= \sigma_s^2V_r(\rho_r)\otimes V_c(\rho_c),
\end{eqnarray}
which is a Kronecker product of two correlation matrices $V_r(\rho_r)$ and $V_c(\rho_c)$ for rows and columns by sorting the residuals columns within rows. 



Let $x_1,\ldots, x_l$ denote the $l$ explanatory variables at the global-level (fixed effects) and $z_1,\ldots, z_k$ denote the $k$ explanatory variables at the local-level (random effects). Suppose we have $n$ measurements from $n$ locations $s_1,\ldots, s_n$ within a study region $\mathcal{S}$. For a given location $s_i\in\mathcal{S}$, the underlying template predictor term $\eta$ of equation \eqref{eq:ynorm} can generally be written as
\begin{equation}\label{eq:predictor}
\eta(s_i)=\sum_{m=1}^{l}b_m x_m(s_i) + \sum_{j=1}^{k}u_j(s_i)z_j(s_i),
\end{equation}
where the unknown $b_m$ and $u_j(s)$ are the coefficients that the latter one is associated with spatial variation, $i=1,\ldots,n$.

In the matrix notation, the model given in above equations \eqref{eq:ynorm} and \eqref{eq:predictor} including all the information is expressed as 
\begin{equation}\label{eq:modelmatrix}
\bm{Y} = \bm{X}\bm{b}+\bm{Z}\bm{u}+\bm{e},
\end{equation}
where $\bm{Y}$ is the $n\times 1$ vector of observation, $\bm{b}$ and $\bm{u}$ are vectors of fixed and random effects , respectively, and $\bm{X}$ and $\bm{Z}$ are the associated design matrices. Hence, the distribution of $\bm{Y}$ is a multivariate distribution with mean $\bm{X}\bm{b}$ and variance $\bm{Z}\Sigma_u\bm{Z}^\top + \Sigma(\sigma)$. 


%% In general cases, the covariance between random effects of different grouping factors are assumed to be zero. With this assumption, the matrix $\bm{Z}$ is split up into a few matrices $\bm{Z}_k$ according to $k$ groups, so that the model will be $\bm{u_k}\sim N(0,\Sigma_k)$. Sometimes we can also assume random effects parameters associated with $j$ levels of the same grouping factor to be independent where $\bm{u_{kj}}\sim N(0,V_k)$ \citep{Burkner2017brms}. 


\subsection{Model with spatially correlated random parameters}

In the context of OFE or precision farming, \cite{Rakshit2020Novel} used the GWR approach and analysed yield monitor data for a corn field in Argentina \citep{Edmondson2014Agridat}. They found that the nitrogen rate is in quadratic form against yield, and the model is estimated through local regression.

As a complementary approach to GWR, we use model \eqref{eq:modelmatrix} by splitting all $\beta$s in GWR into $b$s and $u$s and rewrite the model as
\begin{equation}\label{eq:fullmodel}
y_i = \bm{x}_i^\top\bm{b} + \bm{z}_i^\top\bm{u}_i+e_i,
\end{equation}
where, at any location $s_i\in\mathcal{S}$, $y_i$ is the observation, $\bm{x}_i$ and $\bm{z}_i$ are vectors of treatment effects with three levels, and $e_i$ is a residual error term that $e_i\sim N(0,\sigma_e^2)$. 

Stacking random vectors $\bm{u}_i$ for all $y_i$ at location $s_i$ into a vector $\bm{u}$, we assume that the random vector $\bm{u}$ are from the multivariate normal distribution, and different levels $j$ of the same plot $i$ are correlated. In this case, at any location $s_i$, the covariance matrix of $\bm{u}_i$ becomes $V_u$, which could be a known or unknown parameter in the model. 


Without spatial correlation, the variance matrix of random parameters is
\begin{equation}\label{eq:uncorU}
\Sigma_u = I_n \otimes V_u.
\end{equation}
On the contrary, if the random parameters are spatially correlated with the spatial covariance matrix $V_s$, then the above covariance matrix is re-written as 
\begin{equation}\label{eq:varu}
\Sigma_u = V_s \otimes V_u,
\end{equation}
where $V_s$ is the $AR1(\rho_r)\otimes AR1(\rho_c)$ spatial variance matrix or a weighted distance matrix, and $\Var(\bm{e})=\sigma^2I$. Hence, $\bm{u}\sim N(0,\Sigma_u)$.

Despite the fact that only a single treatment is directly observed at each plot, with this model, the estimation of localised treatment effects $\bm{u}_i$ is possible because the spatial model allows the exploiting of information from neighbouring positions with other treatments \citep{Piepho2011Statistical}. Further, it is better to point out that the above variance nugget $\sigma^2$ could vary on the field, and thus for the term $\sigma^2I$ of a hierarchical model, one may assign a distribution to it, such as $\sigma_i\sim N(\mu_\sigma,sd_\sigma)$, or in an alternative way that it is replaced by a diagonal matrix with $\sigma^2(s_i)$ along the diagonal. Then one may assume that $\bm{\sigma}\sim N(0,\sigma_sI)$. In most scenarios, the latter option only increases the complexity and is not necessary in our study. 



\section{Bayesian process}\label{sec:bayes}

\subsection{Prior specification}

Being different from REML approaches, Bayesian approaches assume parameters are random variables and start the estimation from a prior distribution, which summarises the previous knowledge or the experience before the experiment about the parameters to be estimated \citep{Onofri2019Analysing}. The prior should be clearly defined, even before the experiment has been conducted. 

The selection of priors in Bayesian inference has been discussed for a long time. In usual cases, it is common to use a flat non-informative prior, also known as ``improper prior'' that $p(\theta)\propto \mbox{constant}$, if we know nothing from earlier studies \citep{gelman2006Prior}. In other circumstances, a Cauchy or Gamma prior remains a reasonable candidate. Some researchers prefer inverse Wishart (IW) or inverse Gamma distributions for the standard deviation of a hierarchical model. However, \citep{gelman2017Prior} suggested to use weakly informative priors instead, and half-$t$ family is recommended for standard deviation when the number of groups is small. The general idea is that such a prior affects the information in the likelihood as weakly as possible. 


For the covariance matrices $V_u$, it is possible to use an inverse Wishart distribution \citep{Kass2006Adefault}. However, there are good arguments against this prior and, in some settings, a weakly informative prior is desired. \cite{Sorensen2016Bayesian, McElreath2015Statistical} use the following weakly informative prior for the variance matrix, that 
\begin{align}
V_u &= B(\sigma_u)R_u B(\sigma_u) \\
R_u &\sim \text{LKJcorr}(\epsilon)
\end{align}
where $B(\sigma_u)$ denotes the diagonal matrix with diagonal elements $\sigma_u$, which could be half-$t$, half-Cauchy or inverse-Gamma distribution and $R_u$ is a weak LKJ-prior identity matrix with respect to a positive value $\epsilon$. This statement of prior will adaptively regularise the individual coefficients of random effects and the correlation among them. See \citep{gelman2017Prior, gabry2019Visualization} for some discussions. A $3\times3$ symmetric correlation matrix $R$, for instance, is as  
\begin{equation}
R = \begin{bmatrix}
1 & \rho_{12} &\rho_{13}  \\ \rho_{21} & 1 & \rho_{23} \\ \rho_{31} & \rho_{32} & 1 
\end{bmatrix},
\end{equation}
where $\rho$s are the pairwise correlation parameters. Figure \ref{fig:LKJdensity} demonstrates how the distribution of $\rho$ is influenced by $\epsilon$. Small $\epsilon$ leads a wider tail and big $\epsilon$ narrows down the tail for $\rho$. 

\begin{figure}[!htp]
	\centering	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=6.3cm,width=\linewidth]{Images/LKJdensity}
		\caption{Distribution of correlation coefficients $\rho$ extracted from random $2\times2$ correlation matrices with different $\epsilon$s.}
	\end{subfigure} 
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/LKJdensity2D}
		\caption{Visualisation of $\rho_{12}$ against $\rho_{13}$ from a $3\times3$ correlation matrix with $\epsilon=1$.}
	\end{subfigure}
	\caption{$\text{LKJcorr}(\epsilon)$ probability density. When $\epsilon = 1$, all correlations are equally plausible. As $\epsilon$ increases, extreme correlations become less plausible.}\label{fig:LKJdensity}
\end{figure}



\subsection{Posterior distribution}

%In the context of precision agriculture, farmers and researchers are more interested in applying the optimal treatment, such as nitrogen rate $X_i (i=1,\ldots,n)$, to a particular plot or area that yields the greatest harvest. The target distribution of the optimal treatment 
%\begin{equation}
%p(\bm{X}\mid \bm{Y}) = \int p(\bm{X}\mid \bm{Y},\theta)p(\bm{Y},\theta)d\theta,
%\end{equation}
%is the marginal distribution of a set of unknown parameters $\theta$. 

Suppose that we are interested in estimating $\theta$ from data $Y$ using the probability density $p(\bm{Y}\mid\theta)$, the Bayes theorem tells us that the joint posterior density of the parameters becomes
\begin{equation}
p(\theta\mid \bm{Y}) = \frac{p(\bm{Y}\mid\theta)\pi(\theta)}{p(\bm{Y})},
\end{equation}
where $\pi(\theta)$ is known as the prior distribution that represents our previous knowledge or ``best guess'', $p(\bm{Y}) = \int p(\bm{Y}\mid\theta)\pi(\theta) d\theta$ is the normalising constant and does not affect the inference. Therefore, people prefer to re-write the equation to the form 
\begin{equation}\label{eq:posterior}
p(\theta\mid \bm{Y}) \propto p(\bm{Y}\mid\theta)\pi(\theta),
\end{equation}
which is also called the marginal posterior distribution. The distribution of $p(\theta\mid \bm{Y})$ is the ``Bayesian inference'' of the parameter because all information about $\theta$ is contained in the distribution \citep{Che2010Bayesian}. Then, by taking natural logarithm on the posterior of \eqref{eq:posterior}, we will have
\begin{equation}\label{eq:logposterior}
\ln L(\theta) \propto -\frac{1}{2} (\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})^\top \Sigma(\sigma)^{-1}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u}) +\ln \pi(\theta).
\end{equation}
Because of $\bm{u}\sim N(0,\Sigma_u)$, for faster gradient evaluation and sampling, we impose Cholesky decomposition, such that 
\begin{equation}\label{eq:sigmau}
\Sigma_u = \Sigma(\rho_r)\otimes \Sigma(\rho_c)\otimes V_u = (L_r L_r^\top)\otimes  (L_c L_c^\top)\otimes (L_u L_u^\top) = (L_r\otimes L_c\otimes L_u) (L_r\otimes L_c\otimes L_u)^\top. 
\end{equation}
Therefore,
\begin{equation}\label{eq:LU}
\tilde{\bm{u}} = (L_r\otimes L_c\otimes L_u)(z_r\otimes z_c\otimes z_u) = (L_rz_r)\otimes  (L_cz_c)\otimes (L_uz_u),
\end{equation}
where $z_r, z_c, z_u$ are vectors of \iid random variables sampled from $N(0,1)$. 


\subsection{No U-turn sampler}

Suppose $\theta$ is a set of parameters of interests and $f(\theta)$ is the density function. In Hamiltonian system, a set of auxiliary momentum variables $r$ that are drawn independently from the standard normal distribution is introduced in order to keep the system invariant. Then the joint distribution of $f(\theta,r)$ is 
\begin{equation}
f(\theta,r) =\exp \lbrace -U(\theta)-K(r) \rbrace = \exp \lbrace -H(\theta,r) \rbrace,
\end{equation}
where $H(\theta,r)$ is the Hamiltonian system dynamic equation with potential energy $U(\theta)$ and kinetic energy $K(r)$, which represent the density functions of $\theta$ and $r$ respectively. The property of the dynamics is that it keeps the joint distribution invariant \citep{Nishio2019Performance}. 

When new samples $(\theta^*,r^*)$ are drawn, while Hamiltonian dynamics system is numerically approximated in discrete time space with the leapfrog method to maintain the total energy. The leapfrog methods requires two parameters: the step size $\epsilon$, how far the next draw is from current sample, and $L$, the number of steps to move in the process. When the process is finished, new samples are accepted with probability 
\begin{equation}
\alpha = \min \lbrace 1, \frac{f(\theta^*,r^*)}{ f(\theta,r)} \rbrace. 
\end{equation}

Because HMC is highly sensitive to $\epsilon$ and $L$, \citep{Hoffman2014NoUturn} proposed the No-U-Turn Sampler (NUTS), which determines the step size by adapting it during the warm-up (burn-in) phase to a target acceptance rate, and then used it for all sampling iterations \citep{Monnahan2017Faster}. It also gets rid of $L$ by using the criterion 
\begin{eqnarray}
\frac{d}{dt}\frac{(\theta^*-\theta)\cdot(\theta^*-\theta)}{2}=(\theta^*-\theta)\cdot\frac{d}{dt}(\theta^*-\theta)=(\theta^*-\theta)\cdot r^* <0,
\end{eqnarray}
where $r^*$ is the current momentum and $(\theta^*-\theta)$ is the distance from initial position to current position. The idea is that the trajectory will keep exploring the space until $\theta^*$ starts to move back towards to $\theta$. 


To guarantee time reversibility and to converge to the correct distribution, NUTS overcomes this issue by means of a recursive algorithm that preserves reversibility by running the Hamiltonian simulation both forward and backward in time \citep{Hoffman2014NoUturn}. This process starts with a slice variable $u$ which is uniformly distributed as $p(u\mid\theta, r) = U(0,f(\theta,r) )$, and generates a finite set of all $(\theta,r)$ in the doubling size process by randomly taking forward and backward leapfrog steps until 
\begin{equation}
\begin{matrix}
(\theta^+-\theta^-)\cdot r^- <0 & \mbox{or} & (\theta^+-\theta^-)\cdot r^+ <0,
\end{matrix}
\end{equation}
where $\theta^-, r^-$ and $\theta^+, r^+$ are the leftmost and rightmost of $\theta$ and $r$. The best candidate $(\theta^*,r^*)$ is uniformly sampled form the subset of all candidate $(\theta,r)$. 

Precise definition and description of NUTS algorithm can be found at \citep{Hoffman2014NoUturn}. 



\section{Post-sampling checking}\label{sec:mcmcchain}

%\subsection{Markov chain diagnoses}
%
%Diagnose MCMC chains is essential to monitor whether the chains have converged in the process of posterior sampling. Traditionally, trace plots and auto-correlation functions were applied to diagnose Markov chains. Other than the mentioned methods, \cite{gabry2019Visualization} suggest to use bi-variate scatter plots that mark the divergent transitions and divergence check plots, particularly, for HMC algorithm. 


\cite{gelman2003Bayesian} suggests a few strategies for Bayesian model checks: (1) checking that the posterior inferences are reasonable, given the substantive context of the model; (2) examining the sensitivity of the inferences to reasonable changes in the prior distribution and the likelihood; and (3) checking that the model is capable of generating data like the observed data. One may refer to \citep{gelman2004Exploratory, weiss2016Pediatric, Gelman2013Bayesian, Congdon2019Bayesian} for explanations and applications. We focus the third strategy by graphically checking the replicate and observed data for the reproducible capability of our model. 


In ideal situations, researcher are able to use independent data set which is not used in modelling process to test the performances of different models. Alternatively, one may split one data set into training and testing data sets, and use the former one for model training and latter one for testing. However, it is not feasible for some experimental agricultural data sets. Nevertheless, we can still check the model by using the data that we already have. 


\subsection{Posterior predictive checking}


The posterior predictive checking uses the posterior of the parameter in the model to regenerate the observations. The idea behind the concept is that if a model is a good fit we should be able to use it to generate data that resemble the data that we observed \citep{gabry2019Visualization}. In other words, let $\bm{Y}^{rep}$ denotes the replicate data if the process that generated the data $\bm{Y}$ is replicated with the same value of $\theta$ that generated the observed data. Then $\bm{Y}^{rep}$ is governed by the posterior predictive distribution 
\begin{equation}
p(\bm{Y}^{rep} \mid \bm{Y}) = \int p(\bm{Y}^{rep} \mid \theta)p(\theta \mid \bm{Y}) d\theta. 
\end{equation}
The samples $\bm{Y}^{rep}$ is checked against the data $\bm{Y}$ \citep{dipakdey2005Bayesian, Congdon2019Bayesian}. 


The application of posterior predictive distributions is more robust than prior specification, because the details of the prior are washed out by the likelihood \citep{gelman2017Prior} if the model is good enough.



\subsection{Model diagnosis and evaluation}


Leave-one-out (LOO) cross validation (CV), where one data is omitted at one time and the fitted model based on the remaining data is the best predictor in terms of mean square error, is widely used for model evaluation and selection. In Bayesian statistics, to measure the predictive accuracy, researchers use the expected $\log$ LOO predictive density (ELPD), which is as: 
\begin{equation}\label{eq:elpd}
\mbox{\elpd} = \sum_{i=1}^{n}\log p(y_i\mid y_{-i}),
\end{equation}
where $p(y_i \mid y_{-i}) = \int p(y_i \mid \theta)p(\theta \mid y_{-i})d\theta$ is the LOO predictive density with the $i$-th data omitted from the data set \citep{Vehtari2017Practical}. However, it is too expensive in computing because it requires refitting the model $n$ times. \cite{Burkner2020Efficient} proposed an approximated LOO CV using only a single model fit by instead calculating the pointwise $\log$ predictive density as a fast approximation to the exact LOO CV. It uses the Pareto-smoothed importance-sampling algorithm \citep{Vehtari2017Practical} on the pointwise $\log$-likelihood matrix for each draw $m$ from the full posterior distribution and obtains a PSIS-LOO-CV estimate, which is
\begin{equation}
\widehat{\mbox{\psis}} = \sum_{i=1}^n\log\left(  \frac{ \sum_{m=1}^{M} p(y_i\mid \theta^{(m)})w_i^{(m)} }{ \sum_{m=1}^{M}w_i^{(m)} }\right), 
\end{equation}
where $w_i^{(m)}$ are stabilised weights by PSIS, $m = 1, \ldots, M$.  



The resulting PSIS LOO CV approximations can be used for model diagnosis and comparison. The advantage of PSIS is that it automatically computes an empirical similarity between the full data predictive distribution to the LOO predictive distribution for each left out point \citep{gabry2019Visualization}, and the estimated tail shape parameter $\hat{k}$ of the generalised Pareto distribution can be used for assessing the reliability and approximate convergence rate of the estimates. If $\hat{k}<0.5$ then the distribution of raw importance ratios has finite variance and the central limit theorem holds. But in practice the model is still robust for $\hat{k}$ values up to 0.7. Otherwise the variance and the mean of the raw ratios distribution do not exist \citep{Vehtari2017Practical}. 



%\cite{Vehtari2017Practical} proposed Pareto-smoothed importance sampling (PSIS), which applies a smoothing procedure to the importance weights by fitting a generalised Pareto distribution to the upper tail, to estimate LOO CV. The PSIS estimate of the LOO expected $\log$ pointwise predictive density is 
%\begin{equation}
%\widehat{\mbox{elpd}}_{\mbox{psis-loo}}=\sum_{i=1}^{n}\log  \frac{\sum_{j=1}^{M}\omega_i^j p(y_i\mid \theta^j)}{\sum_{j=1}^{M}\omega_i^j},
%\end{equation}
%where $\omega_i^j$ is the truncated weights in the $j$-th posterior for the $i$-th data.



The Bayesian $R^2$, proposed by \citep{Gelman2019Rsquared}, is used for model evaluation as well. It is the variance of the predicted values divided by the variance of predicted values plus the expected variance of the errors in such way 
\begin{eqnarray}
\mbox{Bayesian } R^2 = \frac{\Var(\bm{Y}^{pred})}{\Var (\bm{Y}^{pred})+\Var(\bm{e})}.
\end{eqnarray}
However, it should not be interpreted solely if the model has a large amount of bad Pareto $\hat{k}$ values which are greater than 0.7 or, even worse, than 1. 



\section{Analysis of Las Rosas data}\label{sec:analysis}


\subsection{Data visualisation}

A part of Las Rosas data set, which is publicly available by the name of \texttt{lasrosas.corn} in the \R-package \texttt{agridat} \citep{Edmondson2014Agridat}, was used in our study. The yield monitor data for a corn field was conducted by incorporating six nitrogen rate treatments in three replicated blocks comprising 18 strips across the field in Argentina. It also consists a topographic factor with four levels: W (West slope), HT (Hilltop), E (East slope) and LO (Low East). An obvious spatial variation in the corn yield can be seen in Figure \ref{fig:lasrossa}. 

%\begin{figure}[!htp]
%	\centering
%	\includegraphics[width=0.4\textwidth,height=6.3cm]{../Plots/lasrossa_view01}
%	\includegraphics[width=0.4\textwidth,height=6cm]{../Plots/lasrossa_view02}
%	\caption{Visualisation of Las Rosas data set.}\label{fig:lasrossa}
%\end{figure}
%\begin{figure}[!htp]
%	\centering
%	\includegraphics[width=0.4\textwidth]{../Plots/view_ridgehist}
%	\includegraphics[width=0.4\textwidth]{../Plots/view_densityhist}
%	\caption{Histogram and density plot of Las Rosas data set}\label{fig:lasrossadensity}
%\end{figure}


\begin{figure}[!htp]
	\centering	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=6.3cm,width=\linewidth]{Images/lasrossa_view01}
		\caption{Visualisation of yield. Yellow colour indicates low yield and dark green indicates high yield.}
	\end{subfigure} 
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/lasrossa_view02}
	    \caption{Coloured yield by topographic factors.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		%\includegraphics[height=6.3cm]{Images/lasrossa_view03}
		%\caption{Histogram of yield by different topographic factors: West slope (W), Hilltop (HT), East slope (E) and Low East (LO).}
		\includegraphics[height=6.3cm,width=\linewidth]{Images/lasrossa_view05}
		\caption{Linear models fitted within each topographic factor: West slope (W), Hilltop (HT), East slope (E) and Low East (LO).}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/lasrossa_view04}
		\caption{Bimodal histogram and density plot of yield.}
	\end{subfigure}
	\caption{Visualisation of Las Rosas yield monitor data for harvests in 2001.}\label{fig:lasrossa}
\end{figure}


In order to analyse the data set, we apply a geographic projection which transforms the geo-spatial coordinates to planar coordinates expressed in meters. The Las Rosas experiment is approximately 810m long and 150m wide \citep{Rakshit2020Novel}.



\subsection{Statistical models and prior predictive simulations}


To demonstrate the power of the proposed model \eqref{eq:fullmodel} which has spatially correlated random parameters $\bm{u}$, we put the model without spatial correlation as the benchmark model. We will also prove that, with the assumption of Gaussian distribution, the model is mis-specified. Instead, we use student-$t$ distribution in the final model. To summarise, we have four models illustrating in Table \ref{tb:models}. 


\begin{table}[!htp]
	\centering
	\begin{tabular}{*{5}{l}} \toprule
		                         & Model 1 & Model 2& Model 3& Model 4  \\ \midrule
	Spatial correlation & No & Yes & No & Yes \\ 
	$\Var(\bm{u})$ &  $I_{n\times n}\otimes V_u$ & $V_s\otimes V_u$ & $I_{n\times n}\otimes V_u$ & $V_s\otimes V_u$ \\ 
	Distribution & Gaussian & Gaussian & Student & Student \\
		\bottomrule
	\end{tabular}\caption{Four models that are fitted in our study.}\label{tb:models}
\end{table}


We start from choosing weakly informative priors for model 1. A weakly informative prior has the capability that samples from the prior distribution through data-generating process could represent any data set that is plausibly observed. 

Vague priors, for instance, are $b_0\sim N(\mu,100)$, $b_1,b_2\sim N(0,100)$ and $\sigma_e\sim IG(1,100)$, where $IG$ is the inverse Gamma distribution. Similarly, we assume $u_{ki}\sim N(0,\sigma_{k}^2)$ with $\sigma_k^2\sim IG(1,100)$ and $k=0, 1, 2$ at location $s_i$. $\mu$ is either mean or median of the observed data. Alternatively, weakly informative priors that we choose are $b_0\sim N(80,10)$, $b_1\sim N(0,0.01)$, $b_2\sim N(0,0.001)$, $\sigma_{0}\sim N_+(0,1)$, $\sigma_{1}\sim N_+(0,0.01)$, $\sigma_{2}\sim N_+(0,0.001)$, $R_u\sim \mbox{LKJcorr}(1)$ and $\sigma_e\sim N_+(0,1)$, where $N_+(\cdot)$ is the positive half Gaussian distribution. 


Figure \ref{fig:priorcheck} compares the regenerated data with vague and weakly informative priors. When the vague priors are applied, model 1 regenerates extremely small and large values which are implausible for yield data. This is mostly because the vague priors disregard practical knowledge. Applying weakly informative priors avoids negative values and keeps the simulations within reasonable interval. Even though some simulations are not perfect, the priors are overall good enough according to our common knowledge. If the priors, on the other hand, are too informative, they may badly influence the posterior distribution and result in partial exploration in the posterior space. 

%where $t$ is the Student-$t$ distribution in the form
%\begin{equation}
%t(y\mid\nu,\mu,\tau) = \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)}\frac{1}{\sqrt{\nu\pi}\tau} \left(1+\frac{1}{\nu}  \left(\frac{y-\mu}{\tau} \right)^2  \right)^{-(\nu+1)/2}
%\end{equation}
%for degrees of freedom $\nu$ and scale parameter $\tau\in\mathcal{R}^+$, mean $\mu$ and $y\in\mathcal{R}$, and $t(\cdot)^+$ indicate a positive half-$t$ distribution with given parameters. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Images/priorcheck_vague}
		\caption{With vague priors}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
	\centering
	\includegraphics[width=\linewidth]{Images/priorcheck_weak}
	\caption{With weakly informative priors}
\end{subfigure}
	\caption{Capability of re-generating observed data with different priors by running 100 simulations. Vague priors failed in regenerating and leads to extreme values. Weakly informative priors give plausible regenerated data. }\label{fig:priorcheck}
\end{figure}



For model 2, besides the priors that are used in model 1, we add the following priors: $\rho_c,\rho_r\sim U(0,1)$ a uniform distribution between 0 and 1. For model 3 and 4, the prior was assigned to the extra parameter of degrees of freedom as $\nu\sim \mbox{Gamma}(2,0.1)$, suggested by \cite{Juarez2010ModelBased} that we keep the prior as easy as possible. 

A summary of the priors are listed in Table \ref{tb:priors}. Please note that it is usually not a good idea to use the same priors for different models, and prior predictive checking is recommended for each model if a new prior is proposed for a new parameter. In out study, with the same weakly informative priors, our models regenerate plausible outcomes, and hence we have confidence that we only need to add extra priors to new parameters and the rest priors stay unchanged. 

Additionally, NUTS does not require conjugate priors, which means the results are still valid if our priors are Gaussian but the data is Student distribution. 


\begin{table}[!htp]
	\centering
	\begin{tabular}{l *{4}{c}} \toprule
                    	  & Model 1 & Model 2& Model 3& Model 4  \\ \midrule
		$b_0$ & \multicolumn{4}{c}{$N(80, 10)$} \\ 
		$b_1$ & \multicolumn{4}{c}{$N(0, 0.01)$} \\ 
		$b_2$ & \multicolumn{4}{c}{$N(0, 0.001)$} \\ 
		$\sigma_0$ & \multicolumn{4}{c}{$N_+(0, 1)$} \\ 
		$\sigma_1$ & \multicolumn{4}{c}{$N_+(0, 0.01)$} \\
		$\sigma_2$ & \multicolumn{4}{c}{$N_+(0, 0.001)$} \\ 
		$\sigma_e$ & \multicolumn{4}{c}{$N_+(0, 1)$} \\ \midrule
		$R_u$ & --- & LKJcorr(1) & --- & LKJcorr(1) \\ 
		$\rho_c$ & --- & $U(0,1)$ & --- & $U(0,1)$ \\ 
		$\rho_r$ & --- & $U(0,1)$ & --- &  $U(0,1)$ \\ 		
		$\nu$ & --- &  --- & $Gamma(2,0.1)$ & $Gamma(2,0.1)$ \\ 
		\bottomrule
	\end{tabular}\caption{Priors of four models.}\label{tb:priors}
\end{table}

With the above priors, the models were run on four parallel Markov chains in \rstan\ with each chain contains 2000 iterations where the first 1000 iterations are in warm-up phase. The final posterior contains 4000 samples for each parameter. 




\subsection{Posterior checking}


The prior predictive checking is a powerful tool for understanding the model. However, it is not able to extend this technique to model selection and performance evaluation. We use MCMC diagnostic tools and posterior predictive checking for further analyses and comparison. 


We firstly use posterior predictive checking to visualise the performance of the four models. Figure \ref{fig:ppcheck} displays the results of the PP checking. If the random parameters are not spatially correlated, model 1 and 3 are incapability in regenerating the data and capture the feature of the observations. But for model 2 and 4, the results are promising. 


\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_GSRNS}
		\caption{PP check of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_GSRand}
		\caption{PP check of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_STRNS}
		\caption{PP check of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_STRand}
		\caption{PP check of Model 4}
	\end{subfigure}
	\caption{Posterior predictive checking for simple linear and the proposed spatial models with 100 simulations.}\label{fig:ppcheck}
\end{figure}


Figure \ref{fig:skewcheck} is the investigation the skewness of the posterior predictive distribution. Model 2 and 4 capture observed skewness, and model 1 and 3 provides, indicated by the plots, are terribly misspecified. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_GSRNS}
		\caption{Skew check of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_GSRand}
		\caption{Skew check of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_STRNS}
		\caption{Skew check of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_STRand}
		\caption{Skew check of Model 4}
	\end{subfigure}
	\caption{Histograms of skewness for 4000 draws from the posterior predictive distribution.}\label{fig:skewcheck}
\end{figure}


To visualise the performance of the models, we use the LOO CV predictive cumulative density plots, which are asymptotically uniform (for continuous data) if the model is well calibrated \citep{gabry2019Visualization, Gelman2013Bayesian}. Figure \ref{fig:pitloo} compare the density of the computed LOO PIT (the thick dark curve) versus 100 simulated data sets from a standard uniform distribution (the thin light curves). Model 1 and 3 are obviously mis-calibrated. The frown shape by model 2 indicates that the model is overall okay but still a little mis-calibrated comparing with Model 4. It is either mis-specified or too flexible. Model 4 performs the best among all four models. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_GSRNS}
		\caption{LOO PIT diagnostic of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_GSRand}
		\caption{LOO PIT diagnostic of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_STRNS}
		\caption{LOO PIT diagnostic of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_STRand}
		\caption{LOO PIT diagnostic of Model 4}
	\end{subfigure}
	\caption{LOO PIT plots of the four models. }\label{fig:pitloo}
\end{figure}


Pareto $\hat{k}$ diagnosis is vital in Bayesian analysis as well. Model 1 has too many large $\hat{k}$ values, which indicates that the model is either misspecified or too flexible. Similarly for model 3 that there are 24 ``bad'' values. These results should not be interpreted solely but together with LOO PIT and \ploo\ values in Table \ref{tb:LOOR}. If we look at Figure \ref{fig:ppcheck}, it is clearly that model 1 and model 3 are misspecified. The LOO PIT plots \ref{fig:pitloo} confirm it even if there are no ``bad'' $\hat{k}$ values. If there are high Pareto $\hat{k}$ values but the model fitting is okay, it means the model is both misspecified and flexible, which indicates that the model has a good capability in predicting unknown data. In this scenario, $K$-fold CV is recommended. 


For model 2, there is one high $\hat{k}$ value, which could be a high influential point or an outlier. However, it still indicates that there might be an issue with this model. Therefore, instead of using Gaussian distribution, model 4 uses Student-$t$ distribution and all $\hat{k}$ values are small (less than 0.7). Then LOO CV does the balancing of goodness-of-fit and parsimony automatically, and \elpd\ and Bayesian $R^2$ are valid. 

\begin{table}[!htp]
	\centering
	\resizebox{\textwidth}{!}{
	\begin{tabular}{ l *{12}{c} } \toprule 
		& \multicolumn{3}{c}{Model 1}   & \multicolumn{3}{c}{Model 2} & \multicolumn{3}{c}{Model 3}   & \multicolumn{3}{c}{Model 4} \\
		       &  Count & Per & M.Eff  &  Count & Per & M.Eff  &  Count & Per & M.Eff  &  Count & Per & M.Eff   \\ \midrule
(-Inf, 0.5] (good)  &  28    &  1.7\%   & 457 &  1673 & 99.9\% & 831&1474 & 88.1\%& 494 & 1673 & 99.9\% & 2990 \\   
(0.5, 0.7] (ok)      &  372  &  22.2\% & 112 &     0  & 0\%        & ---  & 176  & 10.5\% & 254 &   1  & 0.1\% & 589  \\
(0.7, 1] (bad)       &  1138&  68.0\% & 18   &     0 &  0\%        & ---  &  24   &  1.4\%  & 170 &  0    &  0.0\%& ---  \\
(1, Inf) (very bad)&  136  &  8.1\%   & 8     &     1  & 0.1\%      & 11 &  0    &     0\%   & ---   &  0   & 0\%     & ---  \\
		\bottomrule
	\end{tabular}}
	\caption{Pareto $\hat{k}$ diagnostic values including count, percentage (Per) and minimal effective sample sizes (M.Eff) for all models.}\label{tb:Pareto}
\end{table}



\subsection{Model evaluation}


We use ELPD, LOO CV sores and Bayesian $R^2$ to evaluate and compare the performance of different models. \ploo\ is the effective number of parameters and is calculated as the subtraction of \elpd\ and the non-cross-validated $\log$ posterior predictive density. In Bayesian analysis, even if there are no high Pareto $\hat{k}$ values, $R^2$ can’t be trusted if \ploo\ is relatively high compared to the total number of parameters or the number of observations, which indicates weakly predictive capability and probably a model misspecification. 


The results form our study are shown in Table \ref{tb:LOOR}. Other than mean and standard deviation of the posterior distribution, the credibility interval (CI) is reported in the table as well. The level $\alpha$ equal tail credibility interval is the interval bracketed by the $\alpha/2$ and the $1-\alpha/2$ quantiles of the posterior samples, where $\alpha \in (0,1)$. Like frequentist way, we chose a typical $\alpha=0.05$. 


\begin{table}[!htp]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{ l *{8}{c} } \toprule 
		& \multicolumn{2}{c}{Model 1}   & \multicolumn{2}{c}{Model 2} & \multicolumn{2}{c}{Model 3}   & \multicolumn{2}{c}{Model 4} \\
		\midrule
		&  Estimate & SE &  Estimate & SE &  Estimate & SE &  Estimate & SE    \\ 
		\elpd &  -7236.2  & 13.4 & -5253.4 &100.6 &-7848.4 & 17.1 & -5133.0& 37.8 \\   
		\ploo  & 1487.1     & 11.7 & 91.9  &16.6  &241.2  & 6.8 & 97.9 &  2.4 \\
		looic     &  14472.5  & 26.7 & 10506.7 & 201.1 &  15696.8 & 34.3 & 10266.0 & 75.7\\		
		\midrule
		&  Median & CI &   Median & CI &   Median & CI &   Median & CI   \\ 
		Bayesian $R^2$   &  0.842   & 0.563$\sim$0.965 &  0.956  & 0.953$\sim$0.959 & 0.190 & 0.135$\sim$0.251 & 0.974 & 0.970$\sim$0.977 \\
	\bottomrule
\end{tabular}}
\caption{LOO CV estimates with standard errors and medians of Bayesian $R^2$ credibility intervals.}\label{tb:LOOR}
\end{table}


Even if model 1 has higher $R^2$ values compared to model 3, because of a few high Pareto $\hat{k}$ and large \ploo\ values, the $R^2$ should not be interpreted. Then we should only focus on model 2 and 4. Apparently, Student-$t$ is better than Gaussian distribution in terms of smaller LOO CV score and higher $R^2$ value. 




\section{Results}


Table \ref{tb:resultModel4} illustrates the statistics of the posterior distribution of all parameters and Figure \ref{fig:betasprd} display the overall coefficients, fixed terms plus random terms, for the intercept $\hat{\bm{\beta}}_0 = \hat{\bm{b}}_0+\tilde{\bm{u}}_0$, linear term $\hat{\bm{\beta}}_1 = \hat{\bm{b}}_1+\tilde{\bm{u}}_1$ and quadratic term $\hat{\bm{\beta}}_2 = \hat{\bm{b}}_2+\tilde{\bm{u}}_2$. In the Hilltop (middle area) and eastern of West slope (left side) areas, the quadratic term $\bm{\beta}_2$ is negative, which means the optimal treatment level exists. Due to the factors of brightness, sunshine or wind magnitude, the production of yield will decrease if it is over treated. 


\begin{table}[!htp]\centering
	\begin{tabular}{ l *{5}{c}} \toprule
	\multirow{2}{*}[-2pt]{Parameter}  &  \multirow{2}{*}[-2pt]{Mean}   & \multirow{2}{*}[-2pt]{SD}  &   \multicolumn{3}{c}{Credibility interval}  \\  \cmidrule{4-6} 
                           &     &    &    2.5\%   &       Median  &      97.5\% \\ \midrule 
		$\hat{b}_0$   &   88.700  & 2.834 & 83.473 & 88.549 & 94.627   \\
		$\hat{b}_1$   &  0.031 & 0.009 & 0.013 & 0.031& 0.047 \\
		$\hat{b}_2(\times 10^{-4})$   & 1.140 & 0.690 & -0.180 &  1.131 & 2.545  \\
		$\hat{\sigma}_0$&     2.453 & 0.564 & 1.487 &  2.412 &  3.670   \\
		$\hat{\sigma}_1$&   0.011 &  0.005 & 0.004 &  0.010 &  0.022   \\
    	$\hat{\sigma}_2(\times 10^{-4})$&   3.876 &  3.785 &  0.415 &  2.510 & 14.370    \\		
		$\hat{\sigma}_e$& 4.113 & 0.136 & 3.853 & 4.113 & 4.386   \\
		$\hat{\rho}_{12}$& -0.464 & 0.313 & -0.931 & -0.504 &  0.217 \\
		$\hat{\rho}_{13}$& 0.210 &  0.359 & -0.500 &  0.221 &  0.863  \\
		$\hat{\rho}_{23}$& -0.135 & 0.458 & -0.897 & -0.154 &  0.721  \\		
		$\hat{\rho}_r$   &  0.977  & 0.008  & 0.960  & 0.977  & 0.990   \\
		$\hat{\rho}_c$     &   0.990  & 0.007  & 0.973  & 0.992  & 0.998  \\
		$\hat{\nu}$ &  5.40  &  0.783  & 4.122  & 5.318  & 7.140  \\
		\bottomrule
	\end{tabular}\caption{Summary statistics of the posterior samples from model 4.}\label{tb:resultModel4}
\end{table}         


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b0}
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b1}
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b2}
	\caption{Contour plots of overall $\hat{\bm{\beta}}_0$ (top), $\hat{\bm{\beta}}_1$ (middle) and $\hat{\bm{\beta}}_2$ (bottom). }\label{fig:betasprd}
\end{figure}






\subsection{Yield prediction}

With the results from the previous section, we are now able to predict yield with given nitrogen rates. Suppose we apply the medium nitrogen rate 75.4 kg/ha on the entire farm, the yield is calculated by substituting it into the model, where parameters are drawn from the posterior distribution. After a few iterations, we may calculate the median of yield for each grid. The result is shown in Figure \ref{fig:yieldprd}. 


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/STRand_prd}
	\caption{Predicted yield with medium nitrogen rate 75.4 kg/ha across the field.}\label{fig:yieldprd}
\end{figure}



\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/ST_opNitrogen}
	\includegraphics[width=\textwidth]{Images/ST_opYield}
	\caption{Optimal nitrogen rates (top) and estimated yield with the rates (bottom).}\label{fig:optN}
\end{figure}




\subsection{Comparing to GWR}


GWR approach, proposed by \cite{Rakshit2020Novel}, uses geographic weighted local regression to estimate the optimal nitrogen rate for each plot and to predict the yield for OFE. Compared to GWR, the Bayesian approach uses Bayes theorem and NUTS sampler to explore the posterior distribution of the objective and, with Bayesian inference, to estimate the parameters for each plot. 


They are two different paths but aiming at the same goal: providing easy interpretation and visual contour plots for plant growers. Each of these approaches has its own advantages. 


For GWR, the reliability of the results relies on the bandwidth of the moving window. The optimal bandwidth is selected by cross validation. All data points within the window are used for inferring the information of the quarry plot. Other data points, which are out of the window, contributes nothing to the target plot. It has ``higher resolution'' that the optimal treatments at pairwise plots are distinguishing, even though the adjusted-$p$ values are large. On the contrary, the proposed model with Bayesian approach uses all data with the spatial variance matrix on the entire field to estimate the quarry plot. The nearer plots contributes more and further plots contributes less in the inference. So it has ``lower resolution'', which can be seen from Figure \ref{fig:optN}, that the the difference of optimal nitrogen rates in West and East ares are not significant. Additionally, the reliability of the Bayesian approach is affected by the priors and the model itself, whereas the influence of the priors is ``washed out'' if the model is good enough. Additionally, GWR is an ad hoc approach for addressing a particular question. The Bayesian approach has more flexibility to extend and to broadly applied to other questions. 


A comparison of these two approaches is summarised in Table \ref{tb:compareGWR}. 

\begin{table}[!htp]
	\centering
	\begin{tabular}{*{3}{l}} \toprule
		& GWR & Bayesian \\ \midrule
	Inference	& with neighbouring data & with all data \\ 	 
	Initialisation	& bandwidth selection &  prior specification \\ 	
	Objective	& local log-likelihood & global log-likelihood \\ 
	Distinguishability & High & Low \\ \midrule
	Evaluation	&  $t$ scores and $p$-values & credible intervals \\
		&  & PP check and LOO PIT \\
		&  & Pareto $k$ diagnostic \\
		&  & Bayesian $R^2$ \\ 
		\bottomrule
	\end{tabular}
	\caption{Comparison of GWR and Bayesian approach. Each of these two approach has its own advantages and disadvantages.}\label{tb:compareGWR}
\end{table}


\section{Conclusion}


This paper demonstrate the Bayesian framework to analyse spatial varying on farm trial experiment. Starting from the prior selection, we explain the mechanism of Bayesian approach and NUTS sampler. We also present the model checking and diagnostic process for post-sampling. Compared with GWR, Bayesian approach does not require bandwidth selection, but requires pre-specified priors. The results from Bayesian approach are similar with GWR and is able to capture more detailed information on the field. We open our \rstan\ function to public, with which one my apply to their own data sets. 



\section*{Acknowledgement}

SAGI West gratefully acknowledges the support from the Grains Research and Development Corporation of Australia. The computation in this paper has been performed using the \R-packages \rstan. 



\appendix
\section*{Appendix}

\subsection*{Prior predictive checking}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_GSRNS}
		\caption{Model 1: Gaussian distribution without spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_GSRand}
		\caption{Model 2: Gaussian distribution with spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_STRNS}
		\caption{Model 3: Student distribution without spatial correlation.}
    \end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}		
		\includegraphics[width=\linewidth]{Images/prior_STRand}
		\caption{Model 4: Student distribution with spatial correlation.}
	\end{subfigure}
	\caption{Weakly informative priors checking on four models. }\label{fig:priorcheck4models}
\end{figure}



\subsection*{Faster Cholesky factor for $AR1(\rho)$}\label{sec:fastar1}

The $AR1(\rho)$ correlation matrix with correlation factor $\rho$ is defined by $\rho_{ij} = \rho^{|i-j|}$. Not well-known but real helpful that a simple form of Cholesky factor for the $AR1(\rho)$ structure is given by \citep{Madar2015Direct} that the simple form is 
\begin{equation}
l_{ij} = \begin{cases}
\rho^{j-1} & j\geq i =1 \\
\rho^{j-i} \sqrt{1-\rho^2} & j\geq i \geq2
\end{cases}
\end{equation}
which significantly improve the computation efficiency in \rstan. 

\subsection*{Faster Kronecker product}\label{sec:kronec}

Suppose $A=L_A L_A^\top$ and $B=L_B L_B^\top$ are respectively $N\times N$ and $M\times M$ matrices with Cholesky decomposition $L_A$ and $L_B$, Kronecker product has a good property that 
\begin{equation}
C=A\otimes B = (L_A L_A^\top)\otimes (L_B L_B^\top) = (L_A\otimes L_B)(L_A \otimes L_B)^\top,
\end{equation}
where the new matrix $C$ is $NM\times NM$, and the elements of the new matrix $c_{p,q}=a_{i,j}b_{kl}$, where $p=M(i-1)+k$ and $q=M(j-1)+l$. Similarly, the Kronecker product of three matrices, with a $K\times K$ matrix $C$, will be
\begin{equation}
D=A\otimes B\otimes C = (L_A\otimes L_B\otimes L_C) (L_A\otimes  L_B\otimes L_C)^\top,
\end{equation}
and the new elements are $d_{p,q}=a_{i,j}b_{k,l}c_{m,n}$, where $p=K(M(i-1)+k-1)+m$ and $q=K(M(j-1)+l-1)+n$. 

Additionally, with the above properties, we use the following formula to increase the computation efficiency, that
\begin{equation}
(L_A\otimes L_B\otimes L_C)(Z_1\otimes Z_2) = \left((L_A\otimes L_B)Z_1 \right)\otimes (L_C Z_2),
\end{equation}
where $Z_1$ is a vector with length $N+M$ and $Z_2$ is a vector with length $K$. This formula is used in \rstan\ and considerably saves computation time. For interests of other properties of Kronecker product, one can refer to \citep{Zhang2013Kronecker}. 



\bibliographystyle{apalike}
%%\bibliographystyle{plain}
\bibliography{../BayesOFE}


\end{document}