\documentclass[a4paper]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}                   		
\usepackage{graphicx,subcaption}					
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{lineno}
\usepackage{setspace}
\usepackage{booktabs,multirow}
\usepackage{authblk}

\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newcommand{\E}{\mathrm{E}}
\newcommand{\D}{\mathcal{MD}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\R}{\texttt{R}}
\newcommand{\asreml}{\texttt{ASReml-R}}
\newcommand{\brms}{\texttt{brms}}
\newcommand{\rstan}{\texttt{rstan}}
\newcommand{\elpd}{elpd\textsubscript{loo}}
\newcommand{\psis}{elpd\textsubscript{psis-loo}}
\newcommand{\ploo}{p\textsubscript{loo}}

\newcommand{\BigO}[1]{{\rm O}\left(#1\right)}
\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\iid}{\textrm{i.i.d.\ }}
\newcommand{\N}{\mathcal{N}}
\newcommand{\AR}{\mathrm{AR}1}
\newcommand{\Matern}{Mat\'ern }

\usepackage[url=false, isbn=false, eprint=false, backref=true, style= authoryear, backend=bibtex, maxcitenames=2, giveninits=true, maxbibnames=100]{biblatex}
\addbibresource{BayesOFE.bib}

\title{Bayesian Inference of Spatially Correlated Random Parameters for On-farm Experiment}
%\author{Jerome}
\author[1]{Zhanglong Cao}
\author[1]{Katia Stefanova}
\author[1,2]{Mark Gibberd}
\author[1,3]{Suman Rakshit}

\affil[1]{SAGI West, School of Molecular and Life Sciences, Curtin University, Perth, Australia}
\affil[2]{Centre for Crop and Disease Management, School of Molecular and Life Sciences, Curtin University, Perth, Australia}
\affil[3]{School of Electrical Engineering, Computing, and Mathematical Sciences, Curtin University, Perth, Australia}

\date{}							% Activate to display a given date or no date

\linenumbers
\doublespacing
% \onehalfspacing

\begin{document}

\maketitle
	
\begin{abstract}
	Accounting for spatial variability is crucial while estimating treatment effects in large on-farm trials. It allows to determine the optimal treatment for every part of a paddock, resulting in a management strategy that improves sustainability and profitability of the farm. We specify a model with spatially correlated random parameters to account for the spatial variability in large on-farm trials. A Bayesian framework has been adopted to estimate the posterior distribution of these parameters. By accounting for spatial variability, this framework allows the estimation of spatially-varying treatment effects in large on-farm trials. Several approaches have been proposed in the past for assessing spatial variability. However, these approaches lack an adequate discussion of the potential problem of model misspecification. Often the Gaussian distribution is assumed for the response variable, and this assumption is rarely investigated. Using Bayesian post sampling tools, we show how to diagnose the problem of model misspecification. To illustrate the applicability of our proposed method, we analysed a real on-farm strip trial from Las Rosas, Argentina, with the main aim of obtaining a spatial map of optimal nitrogen rates for the entire paddock. The analysis of these data revealed that the assumption of Gaussian distribution for the response variable is unsatisfactory; the Student-$t$ distribution provides a more robust inference. We finish the paper by discussing the difference between the proposed Bayesian approach and geographically weighted regression, and comparing the results of these two approaches. 
\end{abstract}
	
{\bf Keywords:} Geographically weighted regression, Geostatistics, Large strip trials, No-U-turn sampler, Precision agriculture, Site-specific management.
	



\section{Introduction}

Traditional agricultural experiments are often conducted on small plots, and more often than not, these experiments do not address the main concerns of an individual farmer. For a farmer, one of the main motivations of conducting an experiment is to identify a management strategy that could improve the profitability of a farm. This closely aligns with the objective of \emph{site-specific farming}, typically enabled by \emph{precision agriculture} technologies \parencite{Cook1998Precision}. The main aim is to identify the optimal strategy of input utilisation for every part of a large paddock. Because of inherent spatial variation in a large paddock, a uniform management strategy for the entire paddock is sub-optimal. An optimal strategy may require the identification of location-specific optimal treatments that could vary across the paddock. Small plot experiments are inadequate for obtaining a spatially-varying map of optimal treatments for a paddock \parencite{Rakshit2020Novel, Evans2020Assessment}. Consequently, there is an increasing trend to conduct on-farm experiments (OFE) using large strips across farmers' paddocks, utilising their own tools and machinery \parencite{Yan2002Onfarm, Rakshit2020Novel, Evans2020Assessment}. 

Spatial variation in OFE may introduce bias while estimating treatment effects and inflate associated standard errors if not accounted for in fitted models. Spatial variation may be caused by environmental factors such as soil fertility, moisture trends, and light exposure \parencite{Selle2019Flexible}, or it could also arise due to management practices with reoccurring patterns \parencite{Gilmour1997Accounting, Hinkelmann2012Design}. Two common approaches of tackling spatial variation are through the modelling of a \emph{nonstationary} mean structure or modelling of a spatially \emph{autocorrelated} error structure \parencite{Fotheringham2009Problem, Harris2019Simulation}. However, these two forms of spatial variation are quite difficult to disentangle from each other. The following statement from \parencite{Cressie1993Statistics} articulates this point: \textit{``What is one person's (spatial) covariance structure may be another person's mean structure."} 


Our aim in this paper is to obtain spatially-varying estimates of treatment effects, which in turn enables the creation of spatial maps of optimum treatment levels for large paddocks. This further allows an investigation of the central hypothesis of precision agriculture that the optimum treatment  varies spatially within a paddock \parencite{Paez2002General, Brunsdon1999Notes, Lark2003Methoda, Pringle2010Analysis}. To obtain spatially-varying treatment effects, we incorporate spatial heterogeneity in our modelling framework, which is quite different than the traditional models used for analysing small plot trials \parencite{Rakshit2020Novel, Piepho2011Statistical}. The analysis of a small-plot trial typically assumes a spatially-invariant global treatment effect, as the main objective here is to obtain an unbiased estimate of the treatment effect. The unbiased estimation in small plot trials is ensured through appropriate \emph{randomisation} in experimental designs, and the spatial variation is accounted for by fitting a spatially correlated covariance structure to the error terms \parencite{Gilmour1997Accounting, Stefanova2009Enhanced}. Randomisation does not play the same crucial role in the analysis of large strip experiments --- a systematic design is more suitable for estimating spatially-varying treatment effects \parencite{Rakshit2020Novel, Piepho2011Statistical, Evans2020Assessment}. 


We propose a Bayesian framework for modelling the nonstationary first-order effect, characterised by the conditional mean of the response variable, for any location within a paddock. We first specify a regression function with spatially-varying coefficients, representing local departures of treatment effects from their global estimates \parencite{Banerjee2004Hierarchical}. Appropriate \emph{prior} distributions are considered next for the model parameters, and finally, the spatially-varying estimates are computed by sampling from the \emph{posterior} distributions. 


There have been efforts in the recent past to estimate spatially-varying treatment effects for large strip experiments \parencite{Lawes2012Simple, Marchant2019Establishinga, Rakshit2020Novel, Evans2020Assessment}. However, some of these approaches can be considered as merely ad hoc solutions to the problem, particularly restricted to comparing adjacent strips in a large strip trial \parencite{Lawes2012Simple}. A more statistically principled approach, called \emph{geographically weighted regression} (GWR), is proposed by \textcite{Rakshit2020Novel} for estimating spatially-varying treatment effects in large strip experiments, based on the general theory of \emph{local likelihood estimation} \parencite{Hastie1993Local}. GWR is fairly easy to implement using open-source software and provides a pragmatic solution to support on-farm decision making \parencite{Evans2020Assessment}. However, a crucial step in GWR is the bandwidth selection for kernel functions. Inaccurate bandwidth may introduce unknown bias in estimated coefficients. Because the optimal bandwidth size would always be unknown for a given dataset, one needs to use some data-based methods to select an appropriate bandwidth. See \textcite{Rakshit2020Novel} for a discussion on the topic of bandwidth selection for on-farm strip experiments.


The Bayesian framework proposed in this paper simplifies statistical inference by providing straightforward interpretation of the results \parencite{Che2010Bayesian}. Statistical inference using GWR is not straightforward, as it involves adjusting for the problem of multiple testing. In particular, localised $p$-values are required to be adjusted to avoid a large number of false positives in the spatial map of treatment effects; see \textcite{Rakshit2020Novel} for the details of computing adjusted $p$-values in GWR. Due to the availability of adequate computing resources and due to the fact that both model fitting and statistical inference under Bayesian framework are extremely intuitive, Bayesian modelling has become popular for analysing agricultural field trials in the last few years \parencite{Besag1999Bayesian, Theobald2002Bayesian, Che2010Bayesian, Donald2011Bayesian,  Montesinos-Lopez2018Multivariate, Selle2019Flexible, Shirley2020Empirical}. \textcite{Montesinos-Lopez2018Multivariate} proposed a multivariate Bayesian analysis to estimate multiple-trait and multiple-environment on-farm data. \textcite{Selle2019Flexible} compared popular spatial models and proposed a Bayesian modelling framework for variety selection in plant breeding experiments. \textcite{Jiang2009Bayesian} used Bayesian conditional auto-regressive models to account for spatial autocorrelation in OFE data. However, none of these approaches is useful to fit a regression function with spatially-varying coefficients. These methods are also inadequate for developing a management practice that may lead to the optimal use of input resources. 


For modelling spatial nonstationarity, we adopted a Bayesian hierarchical model with spatially correlated random parameters. We use the No-U-Turn Sampler (NUTS) \parencite{Hoffman2014NoUturn} for performing Bayesian inference. NUTS is an efficient sampler that allows quick exploration of the posterior distribution in high dimensional space. NUTS was developed by extending the popular Hamiltonian Monte Carlo (HMC) algorithm to address a crucial drawback of HMC --- it is highly sensitive to two user-specified parameters: a step size $\epsilon$ and the desired number of steps $L$. NUTS determines the step size during the warm-up (burn-in) phase while aiming at a target acceptance rate, and then uses the chosen step size for all subsequent sampling iterations \parencite{Monnahan2017Faster}. It also eliminates the need to set the number of steps $L$; see \cite{Hoffman2014NoUturn} for a detailed discussion on this topic. 

%The HMC is a Markov chain Monte Carlo (MCMC) algorithm that makes use of the gradient of the objective function to reduce random walk behaviour and sensitivity to correlated parameters \parencite{Duane1987Hybrid}.
%\texttt{In order to..... write a reason why it is important faster mixing and convergence. You can make two sentences here as well. Why faster mixing and convergence is useful? The term ``momentum" variable also used without being introduced first. If it is important, then somehow you have to first introduce it. If it is not important, then do not use the term at all.}, 
%It uses ``momentum'' variables that accelerate each iteration within a parameter space to allow faster mixing and convergence \parencite{Ngombe2020Are, Brooks2011Handbook}. A disadvantage of HMC is that it is highly sensitive to two user-specified parameters: a step size $\epsilon$ and the number of steps (\texttt{required to ...}) $L$. Therefore, a poor choice of the latter ((\texttt{poor choice of what? say it clearly! Latter is confusing!})) decreases the efficiency of HMC. 



% \texttt{(Please rewrite this paragraph. Do not say that ``we found that the model with Gaussian distribution is misspecified, and that the influential points/outliers are inevitable". That may be true for the data you have analysed. Say here everything that is true in general, not for a specific dataset. If you want to talk about model misspecification, say that "Using the tools (which you have listed) we can investigate whether the assumption of Gaussian distribution for the response variable is correct. )} 

We investigate the potential problem of model misspecification during the stages of post-sampling posterior diagnoses and model evaluation. To this end, we utilised advanced model diagnostic tools, such as probability integral transformation (PIT) checks \parencite{gabry2019Visualization}, and model evaluation methods, such as Bayesian leave-one-out (LOO) cross validation (CV)  \parencite{Vehtari2017Practical} and Bayesian $R^2$ \parencite{Gelman2019Rsquared}. 


The paper is organised as follows. In Section \ref{sec:model}, we specify the regression model for analysing the data from large strip experiments; in Section \ref{sec:bayes} we describe the prior and posterior distribution for the model, and explain the mechanism of NUTS sampler; in Section \ref{sec:mcmcchain} we discuss the post-sampling model checking and diagnostic process; finally, in Section \ref{sec:analysis} and Section \ref{sec:results}, we apply the proposed model to Las Rosas corn yield data set, and compare with the results obtained from GWR.




\section{Statistical models}\label{sec:model}

% We describe and compare the statistical models with relevance to field experimentation: linear mixed model (LMM) used for the analysis of single and multi-environment trials, conducted on small plots and accommodating spatially correlated random parameters, and 

We describe here a Bayesian hierarchical regression model for analysing data from a large strip experiment.


\subsection{Statistical model for field experiments}\label{sec:fieldmodel}

A field experiment can be considered as a rectangular array, consisting of $r$ rows and $c$ columns, where the total number of the plots (observed data points) in the experiment is $n=r\times c$. We use the notation adopted by \textcite{Zimmerman1991Randoma}, in which $s_i\in \mathcal{R}^2, i=1,\ldots,n$ is a two-cell vector of the Cartesian coordinates of the plot centroid corresponding to the $i$th plot. Let $y(s_i)$ be the dependent variable at a query location/grid $i$. Assume that $\bm{Y}$ is the vector of the plot data ordered as columns within rows, then the matrix notation of the model is 
\begin{equation}\label{eq:modelmatrix}
\bm{Y} = \bm{X}\bm{b}+\bm{Z}\bm{u}+\bm{e},
\end{equation}
where $\bm{b}$ and $\bm{u}$ are vectors of fixed and random effects, respectively, $\bm{X}$ and $\bm{Z}$ are the associated design matrices and $\bm{e}$ is the error vector. We further assume that $\bm{u}$ and $\bm{e}$ are pairwise independent and that their joint distribution is 
\begin{equation}\label{eq:covariance}
\begin{bmatrix}
\bm{u} \\ \bm{e}
\end{bmatrix} \sim N\left( \begin{bmatrix}
0\\0 \end{bmatrix}, \begin{bmatrix}
\Sigma_u & 0 \\ 0 & \Sigma_e
\end{bmatrix}\right),
\end{equation}
and 
\begin{eqnarray}\label{eq:distributionY}
\bm{Y} \sim \N(\bm{X}\bm{b},\bm{Z}\Sigma_u\bm{Z}^\top+\Sigma_e). 
\end{eqnarray}

The structure of the covariance matrix $\Sigma_e$ accommodates a separable spatial model 
\begin{eqnarray}\label{eq:sigma}
\Sigma_e = \sigma^2\Sigma_s,
\end{eqnarray}
where $\Sigma_s$ is some spatial covariance structure and $\sigma^2$ is a nugget variance. \textcite{Gilmour1997Accounting} suggest using a separable first-order autoregressive process $\AR\otimes \AR$ where 
\begin{eqnarray}\label{eq:ar1}
\Sigma_e =\sigma^2\Sigma_c \otimes \Sigma_r
\end{eqnarray}
containing parameters $\rho_c$ and $\rho_r$ for the columns and rows, respectively, and the residuals are ordered rows within columns. In the case where no spatial auto correlation is accounted for, it becomes $\Sigma_e=\sigma^2 I_n$. 

In the classic model for small plot experiments, the variance matrix for $\bm{u}$, assuming $k$ \iid random effects of dimension $m\times 1$ is $G=\oplus_{j=1}^k\sigma_{u_j}^2I_m$, while the model for large strip plots assumes correlated random effect, as discussed below in the next two subsections. 

\subsection{Bayesian hierarchical model}\label{sec:hierarchical}

Each strip within a large strip experiment contains a set of grid points. This setup is considered by \textcite{Rakshit2020Novel}, and these authors proposed the geographically weighted regression (GWR) model for analysing data arising from large strip experiments. GWR allows spatial nonstationarity in modelled relationships and estimates spatially-varying parameters governing these relationships by maximising local loglikelihoods. The regression function defined in GWR can also be written in the form of a linear mixed effects model, given in \eqref{eq:modelmatrix}. However, the main difference in the Bayesian approach is that both model components $\bm{b}$ and $\bm{u}$ are treated similarly, i.e., some (prior) distributions are specified for both $\bm{b}$ and $\bm{u}$, along with the error term \parencite{Burkner2017brms}. In this way, the uncertainty in the estimates of these model parameters can be easily derived using posterior distributions. 


With the same notation given in the previous section, the underlying model is represented as 
\begin{equation}\label{eq:underlying}
\begin{split}
y(s_i)\mid \bm{u}_i,\theta_u,\sigma_e &\sim \mathcal{MD}\left( \sum_{m=1}^{l}b_m x_m(s_i) + \sum_{j=1}^{k}u_j(s_i)z_j(s_i),e(s_i)\right) \\
\bm{u}_i \mid \theta_u &\sim \N(0,V_u(\theta_u))\\
e(s_i) \mid \sigma_e &\sim \N(0,\sigma_e^2) 
\end{split}
\end{equation}
where: $\mathcal{MD}(\cdot)$ is a multivariate distribution assumed to be either multivariate Gaussian or Student $t$; $x_1,\ldots, x_l$ denote $l$ fixed effects and $z_1,\ldots, z_k$ denote $k$ random effects; $b_m$ and $u_j(s)$ are the coefficients for the fixed and random terms, respectively at grid $s_i\in\mathcal{S}$, $i=1,\ldots,n$; $\bm{u}_i$ is a vector of all random effects at $s_i$; $\theta_u$ is a set of parameters of the covariance matrix $V_u$ and $\sigma_e$ a latent variable assumed to be distributed as either Gamma, half-Cauchy or half-normal. 



\subsection{Model with spatially correlated random parameters}


%As a complementary approach to GWR, we use model \eqref{eq:modelmatrix} by splitting all $\beta$s in GWR into $b$s and $u$s and rewrite the model as
%\begin{equation}\label{eq:fullmodel}
%y_i = \bm{x}_i^\top\bm{b} + \bm{z}_i^\top\bm{u}_i+e_i,
%\end{equation}
%where, at any location $s_i\in\mathcal{S}$, $y_i$ is the observation, $\bm{x}_i$ and $\bm{z}_i$ are vectors of treatment effects with three levels, and $e_i$ is a residual error term that $e_i\sim \N(0,\sigma_e^2)$. 
We follow the model description and notation of \textcite{Piepho2011Statistical} where 
%Stacking random vectors $\bm{u}_i$ in \eqref{eq:underlying} for all $y_i$ at location %$s_i$ into a vector $\bm{u}$, 
we assume that the random vector $\bm{u}$ is from a multivariate normal distribution, and the different grids $i$ and $j$ are correlated. Let us assume that at location $s_i$, the covariance matrix of $\bm{u}_i$ is $V_u$. %, which could be a known or unknown parameter in the model. 


Without spatial correlation, the variance matrix of the random parameters is
\begin{equation}\label{eq:uncorU}
\Sigma_u = I_n \otimes V_u.
\end{equation}
If the random parameters are spatially correlated with a spatial covariance matrix $V_s$, then the above covariance matrix is presented as 
\begin{equation}\label{eq:varu}
\Sigma_u = V_s \otimes V_u,
\end{equation}
where $V_s$ is the $\AR(\rho_c)\otimes \AR(\rho_r)$ spatial variance matrix or a weighted distance matrix, $\Var(\bm{e})=\sigma^2I$ and $\bm{u}\sim \N(0,\Sigma_u)$.
The above assumption implies that  the model is simplified by assuming homogeneous error variance $\sigma^2$ across the field. The complexity of the model is already increased by not only assuming a quadratic treatment effect but also varying coefficients of the treatment effects, in accordance with a given spatial process. Additional assumption of heterogeneous error variance increases further the complexity of the model and as illustrated further on the example, did not improve the model. 

Despite that only a single treatment is directly observed at each plot, 
the estimation of localised treatment effects $\bm{u}_i$ is possible due to the fact that the spatial model allows using information from neighbouring plots with other treatments \parencite{Piepho2011Statistical}. 
%Furthermore, it should be  pointed out that the above nugget variance $\sigma^2$ could vary in the field. Therefore, and thus for the term $\sigma^2I$ of a hierarchical model, one may assign a distribution to it, such as $\sigma_i\sim \N(\mu_\sigma,sd_\sigma)$, or in an alternative way that it is replaced by a diagonal matrix with $\sigma^2(s_i)$ along the diagonal. Then one may assume that $\bm{\sigma}\sim \N(0,\sigma_sI)$. In most scenarios, the latter option only increases the complexity and is not necessary in our study. 


\section{Bayesian process}\label{sec:bayes}
 The main focus in the Bayesian approach is to model $f(\theta)$, namely to solve 
\begin{equation}\label{eq:bayesform}
\E[f(\theta)\mid \bm{Y}] = \int_{\Theta} f(\theta)p(\theta\mid \bm{Y})d\theta
\end{equation}
for a given set of parameters $\theta$, using observations $\bm{Y}$. Assuming a prior distribution for $\theta$ and applying the Bayes theorem we obtain the posterior density function $p(\theta\mid \bm{Y})$. The latter leads to the solution of \eqref{eq:bayesform}. 

In this section we discuss the techniques used in the Bayesian approach, applied and illustrated on the Las Rosas data.

\subsection{Prior specification}

The main difference between the REML and Bayesian approaches is that the Bayesian approach assumes that parameters are random variables and runs the estimation from a prior distribution. The latter summarises the previous knowledge for the parameters to be estimated \parencite{Onofri2019Analysing}. The prior is clearly defined, even before the experiment has been conducted. 

The selection of priors in Bayesian inference has been discussed for a long time. Usually, if nothing is known from earlier studies \parencite{gelman2006Prior}, a flat non-informative prior is used, known also as an ``improper prior'', where $p(\theta)\propto \mbox{constant}$. In other circumstances, a Cauchy or Gamma prior remain reasonable candidates. Some researchers prefer inverse Wishart (IW) or inverse Gamma distributions for the standard deviation of a hierarchical model while \textcite{gelman2017Prior} suggested using weakly informative priors instead. In the cases when the number of groups is small, a half-$t$ family is recommended for the standard deviation. The aim is the information in the likelihood to be affected by the prior as weakly as possible. 


A possible choice for the covariance matrices $V_u$ is an inverse Wishart distribution \parencite{Kass2006Default}. However, in our settings, a weakly informative prior is preferred. \textcite{Sorensen2016Bayesian, McElreath2015Statistical} use the following weakly informative prior for the variance matrix 
\begin{align}
V_u &= B(\sigma_u)R_u B(\sigma_u) \\
R_u &\sim \text{LKJcorr}(\epsilon)
\end{align}
where $B(\sigma_u)$ denotes the diagonal matrix with diagonal elements $\sigma_u$, which could be half-$t$, half-Cauchy, inverse-Gamma or half-normal distribution. We adopted the latter. $R_u$ is a weak LKJ-prior identity matrix with respect to a positive value $\epsilon$. This choice of prior will adaptively regularise the individual coefficients of random effects and the correlation among them, see \textcite{gelman2017Prior, gabry2019Visualization} for more details. 




\subsection{Posterior distribution}

In precision agriculture, the focus is on applying the optimal treatment, such as nitrogen rate $X_i (i=1,\ldots,n)$, to a particular plot or area that will lead to the greatest yield $\bm{Y}$ and ultimately the highest profit. In that context, we present the target distribution of the optimal treatment 
\begin{equation}
p(\bm{X}\mid \bm{Y}) = \int p(\bm{X}\mid \bm{Y},\theta)p(\bm{Y},\theta)d\theta,
\end{equation}
as the marginal distribution of a set of unknown parameters $\theta$. 

In order to estimate $\theta$ using $\bm{Y}$, we assuming probability density $p(\bm{Y}\mid\theta)$ and based on the Bayes theorem, we obtain the joint posterior density of the parameters
\begin{equation}
p(\theta\mid \bm{Y}) = \frac{p(\bm{Y}\mid\theta)\pi(\theta)}{p(\bm{Y})},
\end{equation}
where $\pi(\theta)$ is known as the prior distribution that represents our previous knowledge or ``best guess'' and $p(\bm{Y}) = \int p(\bm{Y}\mid\theta)\pi(\theta) d\theta$ is the normalising constant which does not affect the inference. Quite often the equation is presented as 
\begin{equation}\label{eq:posterior}
p(\theta\mid \bm{Y}) \propto p(\bm{Y}\mid\theta)\pi(\theta),
\end{equation}
which is also called the marginal posterior distribution. The distribution of $p(\theta\mid \bm{Y})$ is the ``Bayesian inference'' of the parameter $\theta$ due to the fact that all information about $\theta$ is contained in the distribution \parencite{Che2010Bayesian}. We then take the natural logarithm of the posterior in \eqref{eq:posterior} and obtain for multivariate Gaussian distribution 
\begin{equation}\label{eq:logGpost}
L(\theta) \propto -\frac{1}{2} (\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})^\top \Sigma_e^{-1}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u}) -\frac{1}{2}\ln\det\Sigma_e + \ln \pi(\theta),
\end{equation}
or for multivariate Student-$t$ distribution
\begin{equation}\label{eq:logTpost}
\begin{split}
L(\theta) \propto &-\frac{\nu+n}{2}\ln \left( 
1+\frac{1}{\nu}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})^\top\Sigma_e^{-1}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})  \right) -\frac{n}{2}\ln\nu \\ &+ \ln \Gamma(\frac{\nu+n}{2}) - \ln\Gamma(\frac{\nu}{2})-\frac{1}{2}\ln\det \Sigma_e + \ln \pi(\theta),
\end{split}
\end{equation}
where $\nu$ is the degrees of freedom with distribution $\nu\sim \Gamma^+(2,1)$ for example. 

Assuming $\bm{u}\sim \N(0,\Sigma_u)$, for faster gradient evaluation and sampling we impose Cholesky decomposition, such that 
\begin{equation}\label{eq:sigmau}
\Sigma_u = \Sigma_c\otimes \Sigma_r\otimes V_u = (L_c L_c^\top)\otimes  (L_r L_r^\top)\otimes (L_u L_u^\top) = (L_c\otimes L_r\otimes L_u) (L_c\otimes L_r\otimes L_u)^\top. 
\end{equation}
Therefore,
\begin{equation}\label{eq:LU}
\tilde{\bm{u}} = (L_c\otimes L_r\otimes L_u)(z_c\otimes z_r\otimes z_u) = (L_cz_c)\otimes  (L_rz_r)\otimes (L_uz_u),
\end{equation}
where $z_c, z_r, z_u$ are vectors of \iid random variables sampled from $\N(0,1)$. 


The predictive distribution for a new query location $s^*$, assuming the above posterior distribution, is obtained by marginalisation over $\theta$ and is written as 
\begin{equation}\label{eq:prediction}
p(y(s^*) \mid x(s^*), z(s^*), \bm{Y},\bm{X},\bm{Z}) = \int p(y(s^*) \mid x(s^*), z(s^*), \theta)) p(\theta\mid \bm{Y},\bm{X},\bm{Z}) d\theta. 
\end{equation}


\subsection{No U-turn sampler}

In Hamiltonian Monte Carlo, a set of auxiliary momentum variables $r_d$, drawn independently from the standard normal distribution for each $\theta_d$, is introduced in order to keep the system invariant. The joint density function of $f(\theta,r)$ is 
\begin{equation}
f(\theta,r) \propto \exp \lbrace L(\theta)-K(r) \rbrace = \exp \lbrace -H(\theta,r) \rbrace,
\end{equation}
where $H(\theta,r)$ is the Hamiltonian system dynamics (HSD) equation with potential energy $-L(\theta)$ and kinetic energy $K(r)$, which represent the density functions of $\theta$ and $r$, respectively. The property of the dynamics is that it keeps the joint distribution invariant \parencite{Nishio2019Performance}. 

The HSD is numerically approximated in discrete time space with the leapfrog method to maintain the total energy when new samples $(\theta^*,r^*)$ are drawn. The leapfrog method requires two parameters: the step size $\epsilon$ showing how far the next draw is from the current sample, and $L$ presenting the number of steps to move in the process. The new samples are accepted with probability 
\begin{equation}
\alpha = \min \lbrace 1, \frac{f(\theta^*,r^*)}{ f(\theta,r)} \rbrace. 
\end{equation}

Due to the HMD high sensitivity to the choice of $\epsilon$ and $L$, ultimately badly affecting the results, \textcite{Hoffman2014NoUturn} proposed the No-U-Turn Sampler (NUTS), which determines the step size by adapting it during the warm-up (burn-in) phase to a target acceptance rate, and then use it for all sampling iterations \parencite{Monnahan2017Faster}. The latter also omits $L$ by using the criterion 
\begin{eqnarray}
\frac{d}{dt}\frac{(\theta^*-\theta)\cdot(\theta^*-\theta)}{2}=(\theta^*-\theta)\cdot\frac{d}{dt}(\theta^*-\theta)=(\theta^*-\theta)\cdot r^* <0,
\end{eqnarray}
where $r^*$ is the current momentum and $(\theta^*-\theta)$ is the distance from the initial position to the current position. The idea is that the trajectory will keep exploring the space until $\theta^*$ starts to move back towards $\theta$. 


To guarantee time reversibility and to converge to the correct distribution, NUTS uses a recursive algorithm that preserves reversibility by running the Hamiltonian simulation both forward and backward in time \parencite{Hoffman2014NoUturn}. This process starts with a slice variable $u$ which is uniformly distributed as $p(u\mid\theta, r) = U(0,f(\theta,r) )$, and generates a finite set of all $(\theta,r)$ in the doubling procedure and building binary trees process by randomly taking forward and backward leapfrog steps until 
\begin{equation}
\begin{matrix}
(\theta^+-\theta^-)\cdot r^- <0 & \mbox{or} & (\theta^+-\theta^-)\cdot r^+ <0,
\end{matrix}
\end{equation}
where $\theta^-, r^-$ and $\theta^+, r^+$ are the leftmost and rightmost leaves of $\theta$ and $r$ in the subtree. The best candidate $(\theta^*,r^*)$ is uniformly sampled form the subset of all candidate $(\theta,r)$. 

% Precise definition and description of NUTS algorithm can be found at \textcite{Hoffman2014NoUturn}. 



\section{Post-sampling checking}\label{sec:mcmcchain}

%\subsection{Markov chain diagnoses}
%
%Diagnose MCMC chains is essential to monitor whether the chains have converged in the process of posterior sampling. Traditionally, trace plots and auto-correlation functions were applied to diagnose Markov chains. Other than the mentioned methods, \textcite{gabry2019Visualization} suggest to use bi-variate scatter plots that mark the divergent transitions and divergence check plots, particularly, for HMC algorithm. 


\textcite{gelman2003Bayesian} suggests a few strategies for Bayesian model checks: (1) checking that the posterior inferences are reasonable, given the substantive context of the model; (2) examining the sensitivity of the inferences to reasonable changes in the prior distribution and the likelihood; and (3) checking that the model is capable of generating data like the observed data. One may refer to \textcite{gelman2004Exploratory, Weiss1994Pediatric, Gelman2013Bayesian, Congdon2019Bayesian} for explanations and applications. We focus on the third strategy by graphically checking the replicate and observed data for the reproducible capability of our model. 


In an ideal situation, researchers are able to use independent data set which is not used in the modelling process to test the performances of different models. Alternatively, one may split one data set into training and testing data sets, and use the former one for model training and the latter one for testing. However, it is not feasible for some experimental agricultural data sets. We will illustrate the testing of the model performance using the corn yield data from Las Rosas farm, Rio Cuarto, Cordoba, Argentina, described later. 


\subsection{Posterior predictive checking}


The posterior predictive checking uses the posterior of the parameters in the model to regenerate the observations. The idea behind this concept is that if a model is a good fit we should be able to use it to generate data that resemble the observed data \parencite{gabry2019Visualization}. In order to illustrate it, let assume $\bm{Y}^{rep}$ denotes the replicated data providing the replication process is conducted with the same value of $\theta$ that generated the observed data $\bm{Y}$. Then $\bm{Y}^{rep}$ is governed by the posterior predictive distribution 
\begin{equation}
p(\bm{Y}^{rep} \mid \bm{Y}) = \int p(\bm{Y}^{rep} \mid \theta)p(\theta \mid \bm{Y}) d\theta. 
\end{equation}
The samples $\bm{Y}^{rep}$ are checked against the data $\bm{Y}$ \parencite{dipakdey2005Bayesian, Congdon2019Bayesian}. 


The application of posterior predictive distributions is more robust than prior specification because the details of the prior are washed out by the likelihood \parencite{gelman2017Prior} if the model is good enough.



\subsection{Model diagnosis and evaluation}


Leave-one-out (LOO) cross validation (CV) is widely used for model evaluation and selection. The latter is conducted by omitting one observation at a time, fitting the model based on the remaining data, repeating the process and finally using the average mean square error (MSE) to compare the models. In Bayesian statistics the expected $\log$ LOO predictive density ELPD is used to measure the predictive accuracy : 
\begin{equation}\label{eq:elpd}
\mbox{\elpd} = \sum_{i=1}^{n}\log p(y_i\mid y_{-i}),
\end{equation}
where $p(y_i \mid y_{-i}) = \int p(y_i \mid \theta)p(\theta \mid y_{-i})d\theta$ is the LOO predictive density with the $i$-th data omitted from the data set \parencite{Vehtari2017Practical}. The disadvantage in this measure is the increased computing due to the model being refitted $n$ times. \textcite{Burkner2020Efficient} proposed an approximated LOO CV using only a single model fit and calculating the pointwise $\log$ predictive density as a fast approximation to the exact LOO CV. It uses the Pareto-smoothed importance-sampling algorithm \parencite{Vehtari2017Practical} on the pointwise $\log$-likelihood matrix for each draw $m$ from the full posterior distribution and obtains a PSIS-LOO-CV estimate, which is
\begin{equation}
\widehat{\mbox{\psis}} = \sum_{i=1}^n\log\left(  \frac{ \sum_{m=1}^{M} p(y_i\mid \theta^{(m)})w_i^{(m)} }{ \sum_{m=1}^{M}w_i^{(m)} }\right), 
\end{equation}
where $w_i^{(m)}$ are stabilised weights by PSIS, $m = 1, \ldots, M$.  



The resulting PSIS LOO CV approximations can be used for model diagnosis and comparison. The advantage of PSIS is that it automatically computes an empirical similarity between the full data predictive distribution to the LOO predictive distribution for each left out point \parencite{gabry2019Visualization}. The estimated tail shape parameter $\hat{k}$ of the generalised Pareto distribution can be used for assessing the reliability and approximate convergence rate of the estimates. If $\hat{k}<0.5$ the distribution of raw importance ratios has finite variance and the central limit theorem holds. In practice the model is still robust for $\hat{k}$ values up to 0.7. Otherwise the variance and the mean of the raw ratios distribution do not exist \parencite{Vehtari2017Practical}. 



%\textcite{Vehtari2017Practical} proposed Pareto-smoothed importance sampling (PSIS), which applies a smoothing procedure to the importance weights by fitting a generalised Pareto distribution to the upper tail, to estimate LOO CV. The PSIS estimate of the LOO expected $\log$ pointwise predictive density is 
%\begin{equation}
%\widehat{\mbox{elpd}}_{\mbox{psis-loo}}=\sum_{i=1}^{n}\log  \frac{\sum_{j=1}^{M}\omega_i^j p(y_i\mid \theta^j)}{\sum_{j=1}^{M}\omega_i^j},
%\end{equation}
%where $\omega_i^j$ is the truncated weights in the $j$-th posterior for the $i$-th data.



The Bayesian $R^2$, proposed by \textcite{Gelman2019Rsquared}, is used for model evaluation as well. $R^2$ is presented as the variance of the predicted values divided by the variance of predicted values plus the expected error variance 
\begin{eqnarray}
\mbox{Bayesian } R^2 = \frac{\Var(\bm{Y}^{pred})}{\Var (\bm{Y}^{pred})+\Var(\bm{e})}.
\end{eqnarray}
However, it should not be interpreted solely if the model has a large amount of bad Pareto $\hat{k}$ values i.e. values greater than 0.7 or, even worse, than 1. 



\section{Analysis of Las Rosas data}\label{sec:analysis}

A part of Las Rosas data set, which is publicly available by the name of \texttt{lasrosas.corn} in the \R-package \texttt{agridat} \parencite{Edmondson2014Agridat}, was used in our study. In this section, we adopt the proposed Bayesian approach on the data set. 


\subsection{Data visualisation}

The data were produced by a yield monitor in an Argentinian corn field trial conducted by incorporating six nitrogen rate treatments in three replicated blocks comprising eighteen strips. In order to account for some of the spatial variation (Figure \ref{fig:lasrossa}), a four-level topographic factor was defined: W (West slope), HT (Hilltop), E (East slope) and LO (Low East).  


\begin{figure}[!htp]
	\centering	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=6.3cm,width=\linewidth]{Images/lasrossa_view01}
		\caption{Visualisation of yield. Yellow colour indicates low yield and dark green indicates high yield.}\label{fig:lasrossayield}
	\end{subfigure}
	\space
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/lasrossa_view02}
		\caption{Coloured yield according to the four topographic factors.}\label{fig:lasrossatopo}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		%\includegraphics[height=6.3cm]{Images/lasrossa_view03}
		%\caption{Histogram of yield by different topographic factors: West slope (W), Hilltop (HT), East slope (E) and Low East (LO).}
		\includegraphics[height=6.3cm,width=\linewidth]{Images/lasrossa_view05}
		\caption{Linear models fitted within each topographic factor: West slope (W), Hilltop (HT), East slope (E) and Low East (LO).}\label{fig:lasrossascatter}
	\end{subfigure}
	\space
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/lasrossa_view04}
		\caption{Bimodal histogram and density plot of yield.}\label{fig:lasrossahist}
	\end{subfigure}
	\caption{Visualisation of Las Rosas yield monitor data for harvests in 2001.}\label{fig:lasrossa}
\end{figure}


Additionally, a geographic projection was applied to the data. The latter transforms the geo-spatial coordinates to planar coordinates expressed in meters and assists with the model fitting, see \parencite{Rakshit2020Novel}. The field area of the Las Rosas experiment is approximately 810 metres long and 150 metres wide.



\subsection{Statistical models and prior predictive simulations}


To demonstrate the power of the proposed model \eqref{eq:underlying} where the random parameters $\bm{u}$ are spatially correlated, we compare the model with the one without spatial correlation and used it as a benchmark model. We also compare two distribution assumptions, the Gaussian  with the Student-$t$ distribution in order to decide which better specifies the model. Therefore, we define four models, as illustrated in Table \ref{tb:models}. 


\begin{table}[!htp]
	\centering
	\begin{tabular}{*{5}{l}} \toprule
		& Model 1 & Model 2& Model 3& Model 4  \\ \midrule
		Spatial correlation & No & Yes & No & Yes \\ 
		$\Var(\bm{u})$ &  $I_{n\times n}\otimes V_u$ & $V_s\otimes V_u$ & $I_{n\times n}\otimes V_u$ & $V_s\otimes V_u$ \\ 
		Distribution & Gaussian & Gaussian & Student & Student \\
		\bottomrule
	\end{tabular}\caption{Four models that are fitted in our study.}\label{tb:models}
\end{table}


The modelling process starts by choosing weakly informative priors for model 1. We can generate any data set that is plausibly observed through the model by using weakly informative priors. 

For instance, vague priors are $b_0\sim \N(\mu,100)$, $b_1,b_2\sim \N(0,100)$ and $\sigma_e\sim IG(1,100)$, where $IG$ is the inverse Gamma distribution. Similarly, we assume $u_{ki}\sim \N(0,\sigma_{k}^2)$ with $\sigma_k^2\sim IG(1,100)$ and $k=0, 1, 2$ at grid $s_i$. $\mu$ is either mean or median of the observed data. Alternatively, we can choose weakly informative priors $b_0\sim \N(80,10)$, $b_1\sim \N(0,0.01)$, $b_2\sim \N(0,0.001)$, $\sigma_{0}\sim \N_+(0,1)$, $\sigma_{1}\sim \N_+(0,0.01)$, $\sigma_{2}\sim \N_+(0,0.001)$, $R_u\sim \mbox{LKJcorr}(1)$ and $\sigma_e\sim \N_+(0,1)$, where $\N_+(\cdot)$ is the positive half Gaussian distribution. 

For the correlation matrix $R_u$, using $\mbox{LKJcorr}(\epsilon=1)$ is because we assume that $\bm{u}_i$ from grid $i$ is weakly correlated. To be specific, the $3\times3$ symmetric correlation matrix $R_u$ is  
\begin{equation}
R_u = \begin{bmatrix}
1 & \rho_{12} &\rho_{13}  \\ \rho_{21} & 1 & \rho_{23} \\ \rho_{31} & \rho_{32} & 1 
\end{bmatrix},
\end{equation}
where $\rho$s are the pairwise correlation parameters. Figure \ref{fig:LKJdensity} demonstrates how the distribution of $\rho$ is influenced by $\epsilon$. Small $\epsilon$ leads to a wider tail and big $\epsilon$ narrows down the tail. In the case where $\epsilon = 1$, all correlations are equally plausible. As $\epsilon$ increases, the variables are more likely to be independent. 

\begin{figure}[!htp]
	\centering	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=6.3cm,width=\linewidth]{Images/LKJdensity}
		\caption{Distribution of correlation coefficients $\rho$ extracted from random $2\times2$ correlation matrices with different values of $\epsilon$.}
	\end{subfigure} 
	\space
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/LKJdensity2D}
		\caption{Visualisation of $\rho_{12}$ against $\rho_{13}$ from a $3\times3$ correlation matrix with $\epsilon=1$.}
	\end{subfigure}
	\caption{$\text{LKJcorr}(\epsilon)$ probability density. }\label{fig:LKJdensity}
\end{figure}




Figure \ref{fig:priorcheck} compares the regenerated data with vague and weakly informative priors. When the vague priors are applied, model 1 regenerates extremely small and large values which are implausible for the yield data. This is mostly because the vague priors disregard practical knowledge. Applying weakly informative priors avoids negative values and keeps the simulations within a reasonable interval. Even though some simulations are not perfect, the priors are overall good enough and reflect common knowledge. On the other hand, if the priors are too informative, the posterior distribution maybe badly influenced and result in partial exploration of the posterior space. 

%where $t$ is the Student-$t$ distribution in the form
%\begin{equation}
%t(y\mid\nu,\mu,\tau) = \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)}\frac{1}{\sqrt{\nu\pi}\tau} \left(1+\frac{1}{\nu}  \left(\frac{y-\mu}{\tau} \right)^2  \right)^{-(\nu+1)/2}
%\end{equation}
%for degrees of freedom $\nu$ and scale parameter $\tau\in\mathcal{R}^+$, mean $\mu$ and $y\in\mathcal{R}$, and $t(\cdot)^+$ indicate a positive half-$t$ distribution with given parameters. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Images/priorcheck_vague}
		\caption{With vague priors}
	\end{subfigure}
	\space
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Images/priorcheck_weak}
		\caption{With weakly informative priors}
	\end{subfigure}
	\caption{Capability of re-generating observed data with different priors by running 100 simulations. Vague priors failed in regenerating and lead to extreme values. Weakly informative priors give plausible regenerated data. }\label{fig:priorcheck}
\end{figure}



In model 2, in addition to the priors used in model 1, we use priors: $\rho_c,\rho_r\sim U(0,1)$, a uniform distribution between 0 and 1. In model 3 and 4, an extra parameter of degrees of freedom $\nu\sim \Gamma(2,0.1)$ is assigned to the prior, as suggested by \textcite{Juarez2010ModelBased}, aiming to keep the prior simple. 

A summary of the priors is listed in Table \ref{tb:priors}. It is not recommended to use the same priors for different models. In addition, if a new prior is proposed for a new parameter, checking the prior predictive ability is recommended for each model. In this study, due to the fact that our models regenerated plausible outcomes, we built up the models by using the same weakly informative priors and only adding priors for the new parameters, without changing the rest of the priors.


Additionally, NUTS does not require conjugate priors, which means the results are still valid if our priors are Gaussian but for the data Student distribution is assumed. 


\begin{table}[!htp]
	\centering
	\begin{tabular}{l *{4}{c}} \toprule
		& Model 1 & Model 2& Model 3& Model 4  \\ \midrule
		$b_0$ & \multicolumn{4}{c}{$\N(80, 10)$} \\ 
		$b_1$ & \multicolumn{4}{c}{$\N(0, 0.01)$} \\ 
		$b_2$ & \multicolumn{4}{c}{$\N(0, 0.001)$} \\ 
		$\sigma_0$ & \multicolumn{4}{c}{$\N_+(0, 1)$} \\ 
		$\sigma_1$ & \multicolumn{4}{c}{$\N_+(0, 0.01)$} \\
		$\sigma_2$ & \multicolumn{4}{c}{$\N_+(0, 0.001)$} \\ 
		$\sigma_e$ & \multicolumn{4}{c}{$\N_+(0, 1)$} \\ \midrule
		$R_u$ & --- & LKJcorr(1) & --- & LKJcorr(1) \\ 
		$\rho_c$ & --- & $U(0,1)$ & --- & $U(0,1)$ \\ 
		$\rho_r$ & --- & $U(0,1)$ & --- &  $U(0,1)$ \\ 		
		$\nu$ & --- &  --- & $\Gamma(2,0.1)$ & $\Gamma(2,0.1)$ \\ 
		\bottomrule
	\end{tabular}\caption{Priors of four models.}\label{tb:priors}
\end{table}

With the above priors, the models were run on four parallel Markov chains in \rstan\ with each chain containing 2000 iterations, where the first 1000 iterations are run in the warm-up phase. The final posterior contains 4000 samples for each parameter. 




\subsection{Posterior checking}


The prior predictive checking is a powerful tool for understanding the structure of the model. However, it is not possible to extend this technique to model selection and performance evaluation. We use MCMC diagnostic tools and posterior predictive checking for further analyses and comparison. 


We start with the posterior predictive (PP) checking to visualise the performance of the four models. Figure \ref{fig:ppcheck} displays the results of the PP checking. Appears that if the random parameters are not spatially correlated, model 1 and 3 are incapable of regenerating the data and capturing adequately the distribution of the observations. On contrary, models 2 and 4 show promising results. 


\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_GSRNS}
		\caption{PP check of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_GSRand}
		\caption{PP check of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_STRNS}
		\caption{PP check of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_STRand}
		\caption{PP check of Model 4}
	\end{subfigure}
	\caption{Posterior predictive checking for simple linear and the proposed spatial models with 100 simulations.}\label{fig:ppcheck}
\end{figure}


Figure \ref{fig:skewcheck} illustrates the observed skewness of the posterior predictive distribution for the four models. While models 2 and 4 capture the observed skewness, model 1 and 3 demonstrate model misspecification. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_GSRNS}
		\caption{Skew check of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_GSRand}
		\caption{Skew check of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_STRNS}
		\caption{Skew check of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_STRand}
		\caption{Skew check of Model 4}
	\end{subfigure}
	\caption{Histograms of skewness for 4000 draws from the posterior predictive distribution.}\label{fig:skewcheck}
\end{figure}


LOO CV predictive cumulative density plots are used to visualise the performance of the models. The model is well calibrated when the plots are asymptotically uniform (for continuous data) \parencite{gabry2019Visualization, Gelman2013Bayesian}. Figure \ref{fig:pitloo} compares the density of the computed LOO PIT (the thick dark curve) versus 100 simulated data sets from a standard uniform distribution (the thin light curves). Model 1 and 3 are obviously miscalibrated. Overall, the fit for model 2 is good but the frown shape of the curve indicates  a little miscalibration comparing to Model 4. This implies that the model is either misspecified or too flexible. Among all four models model 4 demonstrates the best fit.

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_GSRNS}
		\caption{LOO PIT diagnosis of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_GSRand}
		\caption{LOO PIT diagnosis of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_STRNS}
		\caption{LOO PIT diagnosis of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_STRand}
		\caption{LOO PIT diagnosis of Model 4}
	\end{subfigure}
	\caption{LOO PIT plots of the four models. }\label{fig:pitloo}
\end{figure}


Pareto $\hat{k}$ diagnostic value is also important. Model 1 has too many large $\hat{k}$ values, which indicates that the model is either misspecified or too flexible. A similar interpretation can be made for model 3 where 24 ``bad'' values can be observed. These results should not be interpreted solely but together with LOO PIT and \ploo\ values in Table \ref{tb:LOOR}. If we look at Figure \ref{fig:ppcheck}, it is clear that models 1 and 3 are misspecified. The LOO PIT plots (Figure \ref{fig:pitloo}) confirm the latter, even if there are no ``bad'' $\hat{k}$ values. In the case where high Pareto $\hat{k}$ values are observed but the model fit is good, one can conclude that the model is both misspecified and flexible. The latter indicates that the model has a good capability in predicting unknown data. In this scenario, $K$-fold CV is recommended. 


For model 2, there is one high $\hat{k}$ value, which could be a high influential point or an outlier. However, it still indicates that there might be an issue with the model. Therefore, instead of using Gaussian distribution, model 4 uses Student-$t$ distribution and all $\hat{k}$ values are small (less than 0.7). Then LOO CV does the balancing of goodness-of-fit and parsimony automatically, and \elpd\ and Bayesian $R^2$ are valid. 

\begin{table}[!htp]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{ l *{12}{c} } \toprule 
			& \multicolumn{3}{c}{Model 1}   & \multicolumn{3}{c}{Model 2} & \multicolumn{3}{c}{Model 3}   & \multicolumn{3}{c}{Model 4} \\
			&  Count & Per & M.Eff  &  Count & Per & M.Eff  &  Count & Per & M.Eff  &  Count & Per & M.Eff   \\ \midrule
			(-Inf, 0.5] (good)  &  28    &  1.7\%   & 457 &  1673 & 99.9\% & 831&1474 & 88.1\%& 494 & 1673 & 99.9\% & 2990 \\   
			(0.5, 0.7] (ok)      &  372  &  22.2\% & 112 &     0  & 0\%        & ---  & 176  & 10.5\% & 254 &   1  & 0.1\% & 589  \\
			(0.7, 1] (bad)       &  1138&  68.0\% & 18   &     0 &  0\%        & ---  &  24   &  1.4\%  & 170 &  0    &  0.0\%& ---  \\
			(1, Inf) (very bad)&  136  &  8.1\%   & 8     &     1  & 0.1\%      & 11 &  0    &     0\%   & ---   &  0   & 0\%     & ---  \\
			\bottomrule
	\end{tabular}}
	\caption{Pareto $\hat{k}$ diagnostic values including count, percentage (Per) and minimal effective sample sizes (M.Eff) for all models.}\label{tb:Pareto}
\end{table}



\subsection{Model evaluation}


We use ELPD, LOO CV scores and Bayesian $R^2$ to evaluate and compare the performance of different models. \ploo\ is the effective number of parameters and is calculated as the subtraction of \elpd\ from the non-cross-validated $\log$ posterior predictive density value. In Bayesian analysis, even if there are no high Pareto $\hat{k}$ values, $R^2$ is not indicative if \ploo\ is relatively high compared to the total number of parameters or the number of observations, which implies weakly predictive capability and probably a model misspecification. 


The results for each model are presented in Table \ref{tb:LOOR}. The mean and standard deviation of the posterior distribution, along with the credibility interval (CI) are reported. The equal tail credibility interval at level $\alpha$ is the interval bracketed by the $\alpha/2$ and the $1-\alpha/2$ quantiles of the posterior samples, where $\alpha \in (0,1)$. We chose $\alpha=0.05$, a standard option by frequentists. 


\begin{table}[!htp]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{ l *{8}{c} } \toprule 
			& \multicolumn{2}{c}{Model 1}   & \multicolumn{2}{c}{Model 2} & \multicolumn{2}{c}{Model 3}   & \multicolumn{2}{c}{Model 4} \\
			\midrule
			&  Estimate & SE &  Estimate & SE &  Estimate & SE &  Estimate & SE    \\ 
			\elpd &  -7236.2  & 13.4 & -5253.4 &100.6 &-7848.4 & 17.1 & -5133.0& 37.8 \\   
			\ploo  & 1487.1     & 11.7 & 91.9  &16.6  &241.2  & 6.8 & 97.9 &  2.4 \\
			looic     &  14472.5  & 26.7 & 10506.7 & 201.1 &  15696.8 & 34.3 & 10266.0 & 75.7\\		
			\midrule
			&  Median & CI &   Median & CI &   Median & CI &   Median & CI   \\ 
			Bayesian $R^2$   &  0.842   & 0.563$\sim$0.965 &  0.956  & 0.953$\sim$0.959 & 0.190 & 0.135$\sim$0.251 & 0.974 & 0.970$\sim$0.977 \\
			\bottomrule
	\end{tabular}}
	\caption{LOO CV estimates with standard errors and medians of Bayesian $R^2$ credibility intervals.}\label{tb:LOOR}
\end{table}


The $R^2$ should not be used solely in the interpretations of the results. For example, even though model 1 has higher $R^2$ values compared to model 3, it also has more terrible high Pareto $\hat{k}$ values and large \ploo\ values, which indicate the misspecification and flexibility. Great flexibility means the model has the power in predicting unknown future data in terms of smaller errors. Therefore, $R^2$ is not indicative for neither model 1 and model 3. The focus should be on models 2 and 4. Apparently, model 4 with Student-$t$ distribution is better than model 2 with Gaussian distribution in terms of smaller LOO CV score and higher $R^2$ value. The ``vary bad'' Pareto $\hat{k}$ value, which indicates a potential outlier, is eliminated by fitting Model 4. 




\section{Results}\label{sec:results}

In the previous section, through model selection and evaluation process, we found that Model 4 fits the data best. It proves the capability of spatially correlated random parameters in capturing the spatial variation. With the Bayesian inference of all parameters and model \eqref{eq:prediction}, we are able to obtain a smooth map showing the optimal level of the treatment across a grid made by rows and columns covering the whole field, and hence the estimated yield map. 



\subsection{Model assessment}
Table \ref{tb:resultModel4} presents the summary statistics of the posterior distribution of all parameters from model 4. It should be noted that the means and the medians for all parameters are very close or identical which indicates robust results. Another feature is the magnitude of the values of $\hat{b}_2$ and $\hat{\sigma}_2$, they are very small. The latter indicates a week influence of the quadratic term of the regression. The pattern of coefficients magnitude is well illustrated in Figure \ref{fig:betasprd}.

\begin{table}[!htp]\centering
	\begin{tabular}{ l *{5}{c}} \toprule
		\multirow{2}{*}[-2pt]{Parameter}  &  \multirow{2}{*}[-2pt]{Mean}   & \multirow{2}{*}[-2pt]{SD}  &   \multicolumn{3}{c}{Credibility interval}  \\  \cmidrule{4-6} 
		&     &    &    2.5\%   &       Median  &      97.5\% \\ \midrule 
		$\hat{b}_0$   &   88.700  & 2.834 & 83.473 & 88.549 & 94.627   \\
		$\hat{b}_1$   &  0.031 & 0.009 & 0.013 & 0.031& 0.047 \\
		$\hat{b}_2(\times 10^{4})$   & 1.140 & 0.690 & -0.180 &  1.131 & 2.545  \\
		$\hat{\sigma}_0$&     2.453 & 0.564 & 1.487 &  2.412 &  3.670   \\
		$\hat{\sigma}_1$&   0.011 &  0.005 & 0.004 &  0.010 &  0.022   \\
		$\hat{\sigma}_2(\times 10^{4})$&   3.876 &  3.785 &  0.415 &  2.510 & 14.370    \\		
		$\hat{\sigma}_e$& 4.113 & 0.136 & 3.853 & 4.113 & 4.386   \\
		$\hat{\rho}_{12}$& -0.464 & 0.313 & -0.931 & -0.504 &  0.217 \\
		$\hat{\rho}_{13}$& 0.210 &  0.359 & -0.500 &  0.221 &  0.863  \\
		$\hat{\rho}_{23}$& -0.135 & 0.458 & -0.897 & -0.154 &  0.721  \\		
		$\hat{\rho}_c$   &  0.977  & 0.008  & 0.960  & 0.977  & 0.990   \\
		$\hat{\rho}_r$   &   0.990  & 0.007  & 0.973  & 0.992  & 0.998  \\
        $\hat{\nu}$ &  5.40  &  0.783  & 4.122  & 5.318  & 7.140  \\
		\bottomrule
	\end{tabular}\caption{Summary statistics of the posterior samples from model 4. Mean, standard deviation (SD), credibility interval and median of posterior samples are reported. }\label{tb:resultModel4}
\end{table}         

Figure \ref{fig:betasprd} displays the overall coefficients of the fixed and random components on three separate plots, for the intercept $\hat{\bm{\beta}}_0 = \hat{\bm{b}}_0+\tilde{\bm{u}}_0$, the linear term $\hat{\bm{\beta}}_1 = \hat{\bm{b}}_1+\tilde{\bm{u}}_1$ and the quadratic term $\hat{\bm{\beta}}_2 = \hat{\bm{b}}_2+\tilde{\bm{u}}_2$, respectively. The plots cover the whole trial area, as presented in Figure \ref{fig:lasrossayield} and \ref{fig:lasrossatopo}. 
The contour maps are aligned with the topology of the area. It can be observed that the Hilltop area and small part of the neighbouring areas on the left and right (see Figure \ref{fig:lasrossatopo}) are exhibiting different pattern in comparison to the other three topological regions, for all of the $\hat{\bm{\beta}}$ coefficients. The linear component coefficient for the Hilltop area is the highest, in the range of 0.14-0.18, while for the other three areas is around 0.02. The latter explains the observed pattern in the contour map.

Furthermore, in the Hilltop (middle area) and eastern of West slope (left side) areas, the quadratic term $\bm{\beta}_2$ is negative, which means the optimal treatment level exists. 

\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b0}
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b1}
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b2}
	\caption{Contour plots of $\hat{\bm{\beta}}_0$ (top), $\hat{\bm{\beta}}_1$ (middle) and $\hat{\bm{\beta}}_2$ (bottom). }\label{fig:betasprd}
\end{figure}



\subsection{Yield prediction}

The results from the previous section allow us to predict yield for any given location with any nitrogen rate. We use the medium nitrogen rate 75.4 kg/ha, as suggested by \textcite{Rakshit2020Novel}, in order to compare the predicted yield across the entire field from both approaches later. After calculating the median yield for each grid, as described above, we obtained the yield map shown in Figure \ref{fig:yieldprd}.


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/STRand_prd}
	\caption{Predicted yield with medium nitrogen rate of 75.4 kg/ha across the field.}\label{fig:yieldprd}
\end{figure}


With the assumption that yield is quadratic response of nitrogen rate, hence, optimal nitrogen rate for each grid is available from the model. However, if the optimum rate exceeds the maximum, the maximum is chose. Statistically, $\hat{N}_i = \min\{ \tilde{N}_i, N_{\mbox{max}}\}$ for $i=1,\ldots,n$. Figure \ref{fig:optN} depicts the map of optimal treatment and estimated yield with the optimal treatment on the field. 


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/ST_opNitrogen_V2.pdf}
	\includegraphics[width=\textwidth]{Images/ST_opYield}
	\caption{Optimal nitrogen rates (top) and estimated yield with the optimal rates (bottom).}\label{fig:optN}
\end{figure}


After the optimal nitrogen applied on the field, the estimated product is improved comparing to the yield with medium nitrogen rate. Figure \ref{fig:stdiff} illustrates the different yield of the two management strategies. The difference is positive and indicates improvement on yield product. 


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/ST_DiffYield}
	\caption{Yield difference of estimated values with optimal nitrogen and with medium nitrogen rate.}\label{fig:stdiff}
\end{figure}


\subsection{Comparing to the GWR approach}\label{sec:comparegwr}

\textcite{Rakshit2020Novel} suggested using geographic weighted regression (GWR) techniques to address the same objective and illustrated the use of a geographic weighted local regression to estimate the optimal nitrogen rate for each plot and to predict the yield for the Las Rosas data set. The Bayesian approach discussed here uses the Bayes theorem, NUTS sampler and Bayesian inference to explore the posterior distribution and the parameters' estimates for each plot. 

The two different approaches aim at the same objective, to providing reliable yield estimates and easy interpretation through visual contour plots, assisting plant growers. Both approaches have their advantages. 

For GWR, the reliability of the results relies on the bandwidth of the moving window. The optimal bandwidth is selected by cross validation. All data points within the window are used for inferring the information of the plot of interest. Other data points, which are out of the window, contribute nothing to the targeted plot. Due to this feature, the data on the boundary of the field are trimmed off, which means the yield and treatment on the boundary are not estimated. GWR has ``higher resolution'' such that the optimal treatments at pairwise plots are distinguishable, even though the adjusted-$p$ values are large. On contrary, the proposed model with a Bayesian approach uses all data with the spatial variance matrix on the entire field to estimate the query plot. The nearer plots contribute more and further plots contribute less to the inference. So it has ``lower resolution'', as it can be seen from Figure \ref{fig:optN} the difference of optimal nitrogen rates in the West and East areas are not significant. Additionally, the reliability of the Bayesian approach is affected by the priors and the model itself, whereas the influence of the priors is ``washed out'' if the model is good enough. Moreover, GWR is an ad hoc approach for addressing a particular question. The Bayesian approach has more flexibility to be extended and to be broadly applied to other questions. 


A comparison of these two approaches is summarised in Table \ref{tb:compareGWR}. 
\begin{table}[!htp]
	\centering
	\begin{tabular}{*{3}{l}} \toprule
		& GWR & Bayesian \\ \midrule
		Inference	& with neighbouring data & with all data \\ 	 
		Initialisation	& bandwidth selection &  prior specification \\ 	
		Objective	& local log-likelihood & global log-likelihood \\ 
		Distinguishability & high & low \\ \midrule
		Evaluation	&  $t$ scores and $p$-values & credible intervals \\
		&  & PP check and LOO PIT \\
		&  & Pareto $k$ diagnosis \\
		&  & Bayesian $R^2$ \\ 
		\bottomrule
	\end{tabular}
	\caption{Comparison of GWR and Bayesian approach.}\label{tb:compareGWR}
\end{table}


\section{Discussion}

In this paper, we demonstrates a Bayesian hierarchical model for the analysis of spatially varying treatment effects in on-farm experiments. We explain the mechanism of a Bayesian approach and the NUTS sampler, which has been proved the ability in sampling highly correlated high-dimensional distributions. NUTS exhibits excellent sampling qualities in terms of generating large effective sample sizes, producing low autocorrelation and obtaining low skewness of marginal posterior distributions \parencite{Nishio2019Performance}. Moreover, NUTS does not require conjugate priors, exhibits faster convergence for multi-parameters and has considerable flexibility for fitting user-specified models by researchers using the \R-package \rstan. However, if the data set is large, computing the inverse of the covariance matrix, which is three times the size of the data, is extremely time consuming by conventional algorithm. Therefore, we implement a faster algorithm for calculating the autocorrelation matrix and develop a faster algorithm for computing the Kronecker product of three matrices. The theory of the algorithms, which can be implemented in \rstan, is presented in the Appendix. 


The \Matern class covariance 
\begin{equation}\label{eq:matcov}
V_s(d) = \sigma^2 \frac{2^{1-\nu}}{\Gamma(\nu)} \left( \sqrt{2\nu} \frac{d}{r}\right)^\nu K_\nu\left( \sqrt{2\nu} \frac{d}{r}\right),
\end{equation}
which is used in spatial analysis \parencite{Cressie1999Classes} and in capturing spatial variation in OFE \parencite{Selle2019Flexible}, can be incorporated in the model as well. %Here, $d$ is the space lag or distance, $r$ is a non-negative scaling parameter, $\nu> 0$ is a smoothness parameter determining the mean-square differentiability of the field, $\sigma_d^2$ is the variance of the process, $\Gamma$ is the Gamma function and $K_\nu$ is the modified Bessel function of the second kind. If $\nu = r + \frac{1}{2}$, then the \Matern covariance can be expressed as a product of an exponential and a polynomial of order $r$. 
However, the difficulty in implementing the \Matern covariance is the huge amount of time in calculating the inverse of the covariance matrix for thousands of times when the data size is large. As a compromise, we have to either wait for a few days to obtain converged MCMC chains or reduce the effective sample size and terminate the sampling process earlier, which takes the risk of achieving non-converged chains and leaving parts of the space unexplored. In fact, the difference of $\AR\otimes\AR$ and \Matern covariance matrices is not significant, shown in \parencite{Selle2019Flexible}. Practically, $\AR\otimes\AR$ covariance is a satisfying choice for both efficiency and accuracy. 



The model checking and diagnostic process for post-sampling were presented as well. The Gaussian assumption of the model for the Las Rosas data is misspecified even though the Bayesian $R^2$ value is relatively high. Alternatively, with the help of the diagnostic tools, we discover that Student-$t$ distribution provides a more robust inference. The Bayesian $R^2$ is misleading in some situations, and should not be interpreted solely. 



Finally, in Section \ref{sec:comparegwr}, we compare with GWR approach. The proposed Bayesian approach does not require bandwidth selection, but requires pre-specified priors. The results from the Bayesian approach are similar to the ones from GWR and the approach is able to capture more detailed information for the field. Additionally, the GWR trims off the edges of the data on the field due to the fact that it requires the neighbouring data to interpolate the query grid with local log-likelihood. On the contrary, the Bayesian approach uses all data to obtain a smooth map. 



\section{Conclusion}

The novelty of our work can be summarised as follows:
\begin{itemize}
	\item Adapts a Bayesian hierarchical model to large on-farm strip trials.
	\item The model consists of a global term and a spatially correlated random term.
	\item The Bayesian inference of all parameters is estimated by faster Kronecker product computing algorithms in \rstan.
	\item Advanced diagnostic tools spot out potential model misspecification. 
	\item Compared with GWR, each approach shows its advantage. 
\end{itemize}

Both the proposed Bayesian and GWR approaches can only fit regular-shaped data set. If zig-zag pattern appear on the edge of the field, or the data are collected in irregular-space, a further investigation should be carried out. 



\section*{Authors’ contribution}

Zhanglong Cao: Conceptualization, Methodology, Writing - Original Draft, Writing - Review \& Editing, Visualization; Katia Stefanova: Writing - Original Draft, Writing - Review \& Editing; Mark Gibberd: Writing - Review \& Editing, Project administration; Suman Rakshit: Conceptualization, Methodology, Writing - Review \& Editing, Supervision. 


\section*{Acknowledgement}

The authors gratefully acknowledge the support from the Grains Research and Development Corporation of Australia (GRDC). 



\appendix
\section*{Appendix}

\subsection*{Prior predictive checking}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_GSRNS}
		\caption{Model 1: Gaussian distribution without spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_GSRand}
		\caption{Model 2: Gaussian distribution with spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_STRNS}
		\caption{Model 3: Student distribution without spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}		
		\includegraphics[width=\linewidth]{Images/prior_STRand}
		\caption{Model 4: Student distribution with spatial correlation.}
	\end{subfigure}
	\caption{Weakly informative priors checking on four models. }\label{fig:priorcheck4models}
\end{figure}



\subsection*{Faster Cholesky factor for $\AR(\rho)$}\label{sec:fastar1}

The $\AR(\rho)$ correlation matrix with correlation coefficient $\rho$ is defined as $\rho_{ij} = \rho^{|i-j|}$. A simple form of Cholesky factor for the $\AR(\rho)$ structure, given by \textcite{Madar2015Direct}, was used 
\begin{equation}
l_{ij} = \begin{cases}
\rho^{j-1} & j\geq i =1 \\
\rho^{j-i} \sqrt{1-\rho^2} & j\geq i \geq2
\end{cases}
\end{equation}
The latter significantly improved the computational efficiency in \rstan. 

\subsection*{Faster Kronecker product}\label{sec:kronec}

Assume $A=L_A L_A^\top$ and $B=L_B L_B^\top$ are respectively $N\times N$ and $M\times M$ matrices with Cholesky decomposition $L_A$ and $L_B$. One good property of the Kronecker product is that
\begin{equation}
C=A\otimes B = (L_A L_A^\top)\otimes (L_B L_B^\top) = (L_A\otimes L_B)(L_A \otimes L_B)^\top,
\end{equation}
where the new matrix $C$ is $NM\times NM$, and the elements of the new matrix are  $c_{p,q}=a_{i,j}b_{k,l}$, where $p=M(i-1)+k$ and $q=M(j-1)+l$. Similarly, the Kronecker product of three matrices, with a $K\times K$ matrix $C$, is
\begin{equation}
D=A\otimes B\otimes C = (L_A\otimes L_B\otimes L_C) (L_A\otimes  L_B\otimes L_C)^\top,
\end{equation}
and the new elements are $d_{p,q}=a_{i,j}b_{k,l}c_{m,n}$, where $p=K(M(i-1)+k-1)+m$ and $q=K(M(j-1)+l-1)+n$. 

Based on the above properties, we use the following formula to increase the computation efficiency
\begin{equation}
(L_A\otimes L_B\otimes L_C)(z_1\otimes z_2\otimes z_3) = (L_A z_1)\otimes (L_B z_2)\otimes (L_C z_3),
\end{equation}
where $z_1$, $z_2$ and $z_3$ are vectors that are \iid sampled from $\N(0,1)$ of length $N$, $M$ and $K$, respectively. The latter was implemented in \rstan\ and considerably saved computation time. For other properties of the Kronecker product see \textcite{Zhang2013Kronecker}.



\renewcommand\bibname{References}% change bibliography title to references
%\addcontentsline{toc}{chapter}{Bibliography}
\addtocontents{toc}{Bibliography}
\printbibliography
\end{document}
