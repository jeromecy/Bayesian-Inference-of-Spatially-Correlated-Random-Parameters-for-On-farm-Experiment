\documentclass[a4paper]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{left=1.5cm,right=1.5cm,top=1.5cm,bottom=1.5cm}                   		
\usepackage{graphicx,subcaption}					
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{lineno}
\usepackage{setspace}
\usepackage{booktabs,multirow}
\usepackage{authblk}

\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newcommand{\E}{\mathrm{E}}
\newcommand{\D}{\mathcal{MD}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\R}{\texttt{R}}
\newcommand{\asreml}{\texttt{ASReml-R}}
\newcommand{\brms}{\texttt{brms}}
\newcommand{\rstan}{\texttt{rstan}}
\newcommand{\elpd}{elpd\textsubscript{loo}}
\newcommand{\psis}{elpd\textsubscript{psis-loo}}
\newcommand{\ploo}{p\textsubscript{loo}}

\newcommand{\BigO}[1]{{\rm O}\left(#1\right)}
\newcommand{\eg}{e.g.\ }
\newcommand{\ie}{i.e.\ }
\newcommand{\iid}{\textrm{i.i.d.\ }}



\usepackage[url=false, isbn=false, eprint=false, backref=true, style= authoryear, backend=bibtex, maxcitenames=2, giveninits=true, maxbibnames=100]{biblatex}
\addbibresource{BayesOFE.bib}

\title{Bayesian Inference of Spatially Correlated Random Parameters for On-farm Experiment}
%\author{Jerome}
\author[1]{Zhanglong Cao}
\author[1]{Suman Rakshit}
\author[1]{Katia Stefanova}
\author[2]{XXX}

\affil[1]{SAGI West}
\affil[2]{\LaTeX\ University}

\date{}							% Activate to display a given date or no date

\linenumbers
%\doublespacing
\onehalfspacing

\begin{document}
\maketitle
	
	
\begin{abstract}
	Accounting for spatial variability is crucial in analysing agricultural trial data, and is more challenging in large on-farm trials than small field trials typically used in agronomy. Different approaches were proposed to assess spatial variation in recent years. However, the potential model misspecification is inadequately discussed in literature. In this paper, we use a Bayesian framework to analyse the model of spatially correlated random parameters for on-farm strip trials and implement the approach using \R\ package \rstan. With advanced post sampling diagnostic tools, we found that the model with Gaussian assumption is misspecified. A more robust Student-$t$ distribution augments the model and avoids misspecification. We also discuss the difference of the Bayesian approach and geographically weighted regression (GWR), and compare the results of the two approaches. The open-source \R\ code is provided for further use.
\end{abstract}
	
{\bf Keywords:} Spatial variation, geostatistics, No-U-turn sampler.
	

\section{Introduction}


Traditional agricultural experiments are quite often conducted on small plots at one or multiple locations for a number of years. However, there is an increasing trend of farmers preference to conduct experiments in their farms. The latter implies the need of adequate statistical models for analysing data collected from on-farm experiments (OFE). \parencite{Yan2002Onfarm}. 



The inherent spatial variation, typical for OFE, may bias treatment estimates and inflate standard errors if not accounted for in the statistical model. The spatial variation is quite often caused by fertility, and/or soil moisture trends, light exposure, and is usually aligned with the rows and columns of the field trial. It could also arise from experimental procedures or management practices that have a reoccurring pattern, known as extraneous variation \parencite{Gilmour1997Accounting, Hinkelmann2012Design}.


\textcite{Selle2019Flexible} compared established spatial models that account for spatial variability in plant breeding experiments. The proposed Bayesian framework and model are good for variety selection, where each variety is replicated two or more times. However, it is incapable of addressing the objective of precision farming: how to maximise the output and/or profit by developing management practices that lead to optimal utilisation of resources across variable environments \parencite{Cook2013Onfarm}. \textcite{Rakshit2020Novel} proposed a geographically weighted regression (GWR) approach to obtain spatially-varying estimates of treatment effects for OFE along with relevant inference, presented by local $t$-scores and $p$-values. A crucial step in their approach is the bandwidth selection for kernel functions. Inaccurate selection affects the estimation and causes biases. 



In this paper, we adopted a Bayesian approach to fit the model that explores spatial nonstationarity by incorporating spatially correlated random parameters. This approach simplifies the interpretation of the results and improves the inference \parencite{Che2010Bayesian}. Compared with restricted maximum likelihood (REML) approach, the Bayesian approach demonstrates advantages in terms of variation control and powerful inference \parencite{Omer2017Comparing, Guan2017Estimation}.


We use the No-U-Turn Sampler (NUTS) \parencite{Hoffman2014NoUturn}, which allows better exploration of the posterior distribution. NUTS is an extension to the Hamiltonian Monte Carlo (HMC) and is adopted in the \R\ package \rstan. The HMC, originally called hybrid Monte Carlo \parencite{Duane1987Hybrid}, is a MCMC algorithm that makes use of gradient of the objective function to reduce random walk behaviour. It uses ``momentum'' variables that accelerate each iteration within a parameter space to allow faster mixing and convergence \parencite{Ngombe2020Are, Brooks2011Handbook}. A disadvantage of HMC is that it is highly sensitive to two user-specified parameters: a step size $\epsilon$ and a desired number of steps $L$. Therefore, a poor choice of the latter decreases the efficiency of HMC. Alternatively, NUTS allows to determine the step size during the warm-up (burn-in) phase while aiming at a target acceptance rate, and then uses it for all sampling iterations \parencite{Monnahan2017Faster}. It is an useful sampling method due to its good sampling qualities: large effective sample sizes, low auto-correlations, and low skewness of marginal posterior distributions \parencite{Nishio2019Performance}. Moreover, NUTS does not require conjugate property on priors, exhibits faster convergence for multi-parameters and has higher flexibility for self-defined models by researchers.


Furthermore, we estimate the posterior distribution of the model that incorporates spatially correlated random parameters for OFE data. With advanced model diagnostic tools, such as probability integral transformation (PIT) checks \parencite{gabry2019Visualization}, and evaluation methods, such as Bayesian leave-one-out (LOO) cross validation (CV) for model comparison \parencite{Vehtari2017Practical}, we found that the model with Gaussian distribution is misspecified, and that the influential points/outliers are inevitable. The latter justified a search for improvement and we found that the use of Student-$t$ distribution significantly improvement the model and consequently the results. 

The objective of this paper is to demonstrate the capability and use of the Bayesian approach in modelling spatial nonstationarity and analysing spatially correlated data for large field plots, typical for OFE. The paper is organised as follow: in Section \ref{sec:model} we propose a generic spatial model for OFE data; in Section \ref{sec:bayes} we discuss the prior and posterior distribution for the model, and explained the mechanism of NUTS sampler; in Section \ref{sec:mcmcchain} we discuss the post-sampling model checking and diagnostic process; finally, in Section \ref{sec:analysis} we apply the proposed model and Bayesian framework to Las Rosas corn yield data set, and compare it with GWR.  Our \rstan\ script is open to public and can be applied to similar data by other users.



\section{Statistical models}\label{sec:model}

We describe the statistical models with relevance to filed experimentation: linear mixed model (LMM) used for the analysis of single and multi-environment trials, conducted on small plots and accommodating spatially correlated random parameters, and Bayesian hierarchical models allowing to model the spatial variability for large strip plots, typical for OFE.   
\textcite{Piepho2011Statistical} summarise three general options regarding the range of spatial correlation for modelling geo-referenced measurements: spatial correlation within but not between plots \parencite{Ritter2008AnOnfarm}, spatial correlation across a whole block but not between blocks \parencite{Piepho2008Nearest} and spatial correlation across the whole field \parencite{Hong2005Spatial, Hurley2004Estimating}. For the OFE model discussed here, we focus on the third option. 


\subsection{Statistical model for field experiments}\label{sec:fieldmodel}

A field experiment can be considered as a rectangular array, consisting of $r$ rows and $c$ columns, where the total number of the plots in the experiment is $n=r\times c$. We adopt \textcite{Zimmerman1991Randoma} notation where $s_i\in \mathcal{R}^2, i=1,\ldots,n$ is a two-cell vector of the Cartesian coordination of the plot centroids, located on a regular grid. Let $y(s_i)$ be the dependent variable at a query location/grid $i$. Assume that $\bm{Y}$ is the vector of the plot data ordered as columns within rows, then the matrix notation of the model is 
\begin{equation}\label{eq:modelmatrix}
\bm{Y} = \bm{X}\bm{b}+\bm{Z}\bm{u}+\bm{e},
\end{equation}
where $\bm{b}$ and $\bm{u}$ are vectors of fixed and random effects, respectively, $\bm{X}$ and $\bm{Z}$ are the associated design matrices and $\bm{e}$ is the error vector. We further assume that $\bm{u}$ and $\bm{e}$ are pairwise independent and that their joint distribution is 
\begin{equation}\label{eq:covariance}
\begin{bmatrix}
\bm{u} \\ \bm{e}
\end{bmatrix} \sim N\left( \begin{bmatrix}
0\\0 \end{bmatrix}, \begin{bmatrix}
\Sigma_u & 0 \\ 0 & \Sigma_e
\end{bmatrix}\right),
\end{equation}
and 
\begin{eqnarray}\label{eq:distributionY}
\bm{Y} \sim N(\bm{X}\bm{b},\bm{Z}\Sigma_u\bm{Z}^\top+\Sigma_e). 
\end{eqnarray}

The structure of the covariance matrix $\Sigma_e$ accommodates a separable spatial model that 
\begin{eqnarray}\label{eq:sigma}
\Sigma_e = \sigma^2\Sigma_s,
\end{eqnarray}
where $\Sigma_s$ is some spatial covariance structure and $\sigma^2$ is a nugget variance. \textcite{Gilmour1997Accounting} suggest to use a separable first-order autoregressive process $AR1\times AR1$ where 
\begin{eqnarray}\label{eq:ar1}
\Sigma_e =\sigma^2\Sigma_r \otimes \Sigma_c
\end{eqnarray}
containing parameters $\rho_r$ and $\rho_c$ for the rows and columns directions, respectively, and the residuals are ordered columns within rows. In the case where no spatial auto correlation is accounted for, it becomes $\Sigma_e=\sigma^2 I_n$. 

In the classic model for small plot experiments, the variance matrix for $\bm{u}$, assuming $k$ \iid random effects of dimension $m\times 1$ is $G=\oplus_{j=1}^k\sigma_{u_j}^2I_m$, while the model for large strip plots assumes correlated random effect, as discussed further in the paper. 

\subsection{Bayesian hierarchical model}\label{sec:hierarchical}

Each large strip plot, typical for OFE or precision farming, could be considered as a set of grids while each plot is a single grid for small plot experiments. The former were considered by \textcite{Rakshit2020Novel} and the geographically weighted regression (GWR) model was used. The latter allows spatial nonstationarity in modelled relationships and estimates spatially-varying parameters governing these relationships. We extend the GWR model by including fixed and random components and align it with the notation of \eqref{eq:modelmatrix}. Besides, being different from maximum likelihood approaches, Bayesian approaches treat $\bm{u}$ as model parameters in the same manner as $\bm{b}$ rather than assessing it along with the error term \parencite{Burkner2017brms}. In this way, the uncertainty in its estimates can be naturally evaluated instead of being derived from \eqref{eq:distributionY}. 


With the same notation given in the previous section, the underlying model is represented as 
\begin{equation}\label{eq:underlying}
\begin{split}
y(s_i)\mid \bm{u}_i,\theta_u,\sigma_e &\sim \mathcal{MD}\left( \sum_{m=1}^{l}b_m x_m(s_i) + \sum_{j=1}^{k}u_j(s_i)z_j(s_i),e(s_i)\right) \\
\bm{u}_i \mid \theta_u &\sim N(0,V_u(\theta_u))\\
e(s_i) \mid \sigma_e &\sim N(0,\sigma_e^2) 
\end{split}
\end{equation}
where: $\mathcal{MD}(\cdot)$ is a multivariate distribution assumed to be either multivariate Gaussian or Student $t$; $x_1,\ldots, x_l$ denote $l$ fixed effects and $z_1,\ldots, z_k$ denote $k$ random effects; $b_m$ and $u_j(s)$ are the coefficients for the fixed and random terms, respectively at grid $s_i\in\mathcal{S}$, $i=1,\ldots,n$; $\bm{u}_i$ is a vector of all random effects at $s_i$; $\theta_u$ is a set of parameters of the covariance matrix $V_u$ and $\sigma_e$ a latent variable assumed to be distributed as either Gamma, half-Cauchy or half-normal. 



\subsection{Model with spatially correlated random parameters}


%As a complementary approach to GWR, we use model \eqref{eq:modelmatrix} by splitting all $\beta$s in GWR into $b$s and $u$s and rewrite the model as
%\begin{equation}\label{eq:fullmodel}
%y_i = \bm{x}_i^\top\bm{b} + \bm{z}_i^\top\bm{u}_i+e_i,
%\end{equation}
%where, at any location $s_i\in\mathcal{S}$, $y_i$ is the observation, $\bm{x}_i$ and $\bm{z}_i$ are vectors of treatment effects with three levels, and $e_i$ is a residual error term that $e_i\sim N(0,\sigma_e^2)$. 
We follow the model description and notation of \textcite{Piepho2011Statistical} where 
%Stacking random vectors $\bm{u}_i$ in \eqref{eq:underlying} for all $y_i$ at location %$s_i$ into a vector $\bm{u}$, 
we assume that the random vector $\bm{u}$ is from a multivariate normal distribution, and the different grids $i$ and $j$ are correlated. Let us assume that at location $s_i$, the covariance matrix of $\bm{u}_i$ is $V_u$. %, which could be a known or unknown parameter in the model. 


Without spatial correlation, the variance matrix of the random parameters is
\begin{equation}\label{eq:uncorU}
\Sigma_u = I_n \otimes V_u.
\end{equation}
If the random parameters are spatially correlated with a spatial covariance matrix $V_s$, then the above covariance matrix is presented as 
\begin{equation}\label{eq:varu}
\Sigma_u = V_s \otimes V_u,
\end{equation}
where $V_s$ is the $AR1(\rho_r)\otimes AR1(\rho_c)$ spatial variance matrix or a weighted distance matrix, $\Var(\bm{e})=\sigma^2I$ and $\bm{u}\sim N(0,\Sigma_u)$.
The above assumption implies that  the model is simplified by assuming homogeneous error variance $\sigma^2$ across the field. The complexity of the model is already increased by not only assuming a quadratic treatment effects but also varying coefficients of the treatment effects, in accordance with a given spatial process. Additional assumption of heterogeneous error variance increases further the complexity of the model and as illustrated further on the example, did not improve the model. 

Despite that only a single treatment is directly observed at each plot, 
the estimation of localised treatment effects $\bm{u}_i$ is possible due to the fact that the spatial model allows using information from neighbouring plots with other treatments \parencite{Piepho2011Statistical}. 
%Furthermore, it should be  pointed out that the above nugget variance $\sigma^2$ could vary in the field. Therefore, and thus for the term $\sigma^2I$ of a hierarchical model, one may assign a distribution to it, such as $\sigma_i\sim N(\mu_\sigma,sd_\sigma)$, or in an alternative way that it is replaced by a diagonal matrix with $\sigma^2(s_i)$ along the diagonal. Then one may assume that $\bm{\sigma}\sim N(0,\sigma_sI)$. In most scenarios, the latter option only increases the complexity and is not necessary in our study. 


\section{Bayesian process}\label{sec:bayes}

A general Bayesian process is in the interest of 
\begin{equation}\label{eq:bayesform}
\E[f(\theta)\mid \bm{Y}] = \int_{\Theta} f(\theta)p(\theta\mid \bm{Y})d\theta
\end{equation}
for a model $f(\theta)$ with a set of parameter $\theta$ and observation $\bm{Y}$. When a prior distribution of $\theta$ is incorporated, Bayes theorem leads us to the posterior density function $p(\theta\mid \bm{Y})$, and, with which, numerically yields to the solution of \eqref{eq:bayesform}. 

In this section we discuss the techniques used in the Bayesian approach, applied and illustrated on the Las Rosas data.

\subsection{Prior specification}

The main difference between the REML and Bayesian approaches is that the Bayesian approach assumes that the parameters are random variables and runs the estimation from a prior distribution. The latter summarises the previous knowledge for the parameters to be estimated \parencite{Onofri2019Analysing}. The prior should be clearly defined, even before the experiment has been conducted. 

The selection of priors in Bayesian inference has been discussed for a long time. In usual cases, it is common to use a flat non-informative prior, also known as ``improper prior'' that $p(\theta)\propto \mbox{constant}$, if we know nothing from earlier studies \parencite{gelman2006Prior}. In other circumstances, a Cauchy or Gamma prior remain reasonable candidates. Some researchers prefer inverse Wishart (IW) or inverse Gamma distributions for the standard deviation of a hierarchical model. However, \textcite{gelman2017Prior} suggested to use weakly informative priors instead, and half-$t$ family is recommended for the standard deviation when the number of groups is small. The main idea is that the prior affects the information in the likelihood as weakly as possible. 


For the covariance matrices $V_u$, it is possible to use an inverse Wishart distribution \parencite{Kass2006Adefault}. However, there are good arguments against this prior and, in some settings, a weakly informative prior is desired. \textcite{Sorensen2016Bayesian, McElreath2015Statistical} use the following weakly informative prior for the variance matrix, that 
\begin{align}
V_u &= B(\sigma_u)R_u B(\sigma_u) \\
R_u &\sim \text{LKJcorr}(\epsilon)
\end{align}
where $B(\sigma_u)$ denotes the diagonal matrix with diagonal elements $\sigma_u$, which could be half-$t$, half-Cauchy or inverse-Gamma distribution and $R_u$ is a weak LKJ-prior identity matrix with respect to a positive value $\epsilon$. This statement of prior will adaptively regularise the individual coefficients of random effects and the correlation among them. See \textcite{gelman2017Prior, gabry2019Visualization} for some discussions. A $3\times3$ symmetric correlation matrix $R$, for instance, is presented as  
\begin{equation}
R = \begin{bmatrix}
1 & \rho_{12} &\rho_{13}  \\ \rho_{21} & 1 & \rho_{23} \\ \rho_{31} & \rho_{32} & 1 
\end{bmatrix},
\end{equation}
where $\rho$s are the pairwise correlation parameters. Figure \ref{fig:LKJdensity} demonstrates how the distribution of $\rho$ is influenced by $\epsilon$. Small $\epsilon$ leads a wider tail and big $\epsilon$ narrows down the tail for $\rho$. In the case where $\epsilon = 1$, all correlations are equally plausible. As $\epsilon$ increases, extreme correlations become less plausible.

\begin{figure}[!htp]
	\centering	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=6.3cm,width=\linewidth]{Images/LKJdensity}
		\caption{Distribution of correlation coefficients $\rho$ extracted from random $2\times2$ correlation matrices with different values of $\epsilon$.}
	\end{subfigure} 
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/LKJdensity2D}
		\caption{Visualisation of $\rho_{12}$ against $\rho_{13}$ from a $3\times3$ correlation matrix with $\epsilon=1$.}
	\end{subfigure}
	\caption{$\text{LKJcorr}(\epsilon)$ probability density. }\label{fig:LKJdensity}
\end{figure}



\subsection{Posterior distribution}

%In the context of precision agriculture, farmers and researchers are more interested in applying the optimal treatment, such as nitrogen rate $X_i (i=1,\ldots,n)$, to a particular plot or area that yields the greatest harvest. The target distribution of the optimal treatment 
%\begin{equation}
%p(\bm{X}\mid \bm{Y}) = \int p(\bm{X}\mid \bm{Y},\theta)p(\bm{Y},\theta)d\theta,
%\end{equation}
%is the marginal distribution of a set of unknown parameters $\theta$. 

Suppose that we are interested in estimating $\theta$ from data $\bm{Y}$ using the probability density $p(\bm{Y}\mid\theta)$, the Bayes theorem tells us that the joint posterior density of the parameters becomes
\begin{equation}
p(\theta\mid \bm{Y}) = \frac{p(\bm{Y}\mid\theta)\pi(\theta)}{p(\bm{Y})},
\end{equation}
where $\pi(\theta)$ is known as the prior distribution that represents our previous knowledge or ``best guess'', $p(\bm{Y}) = \int p(\bm{Y}\mid\theta)\pi(\theta) d\theta$ is the normalising constant and does not affect the inference. Quite often the equation is presented as 
\begin{equation}\label{eq:posterior}
p(\theta\mid \bm{Y}) \propto p(\bm{Y}\mid\theta)\pi(\theta),
\end{equation}
which is also called the marginal posterior distribution. The distribution of $p(\theta\mid \bm{Y})$ is the ``Bayesian inference'' of the parameter $\theta$ because all information about $\theta$ is contained in the distribution \parencite{Che2010Bayesian}. Then, by taking natural logarithm on the posterior of \eqref{eq:posterior}, for multivariate Gaussian distribution, we will obtain
\begin{equation}\label{eq:logGpost}
L(\theta) \propto -\frac{1}{2} (\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})^\top \Sigma_e^{-1}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u}) -\frac{1}{2}\ln\det\Sigma_e + \ln \pi(\theta),
\end{equation}
or for multivariate Student $t$ distribution
\begin{equation}\label{eq:logTpost}
\begin{split}
L(\theta) \propto &-\frac{\nu+n}{2}\ln \left( 
1+\frac{1}{\nu}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})^\top\Sigma_e^{-1}(\bm{Y}-\bm{X}\bm{b}-\bm{Z}\bm{u})  \right) -\frac{n}{2}\ln\nu \\ &+ \ln \Gamma(\frac{\nu+n}{2}) - \ln\Gamma(\frac{\nu}{2})-\frac{1}{2}\ln\det \Sigma_e + \ln \pi(\theta),
\end{split}
\end{equation}
where $\nu$ is the degrees of freedom with distribution $\nu\sim Gamma^+(2,1)$ for example. 

Assuming $\bm{u}\sim N(0,\Sigma_u)$, for faster gradient evaluation and sampling we impose Cholesky decomposition, such that 
\begin{equation}\label{eq:sigmau}
\Sigma_u = \Sigma_r\otimes \Sigma_c\otimes V_u = (L_r L_r^\top)\otimes  (L_c L_c^\top)\otimes (L_u L_u^\top) = (L_r\otimes L_c\otimes L_u) (L_r\otimes L_c\otimes L_u)^\top. 
\end{equation}
Therefore,
\begin{equation}\label{eq:LU}
\tilde{\bm{u}} = (L_r\otimes L_c\otimes L_u)(z_r\otimes z_c\otimes z_u) = (L_rz_r)\otimes  (L_cz_c)\otimes (L_uz_u),
\end{equation}
where $z_r, z_c, z_u$ are vectors of \iid random variables sampled from $N(0,1)$. 


With the above posterior distribution, the predictive distribution for a new query location $s^*$ can be obtained by marginalisation over $\theta$ and written as 
\begin{equation}\label{eq:prediction}
p(y(s^*) \mid x(s^*), z(s^*), \bm{Y},\bm{X},\bm{Z}) = \int p(y(s^*) \mid x(s^*), z(s^*), \theta)) p(\theta\mid \bm{Y},\bm{X},\bm{Z}) d\theta. 
\end{equation}


\subsection{No U-turn sampler}

In Hamiltonian Monte Carlo, a set of auxiliary momentum variables $r_d$, that are drawn independently from the standard normal distribution for each $\theta_d$, is introduced in order to keep the system invariant. Then the joint density function of $f(\theta,r)$ is 
\begin{equation}
f(\theta,r) \propto \exp \lbrace L(\theta)-K(r) \rbrace = \exp \lbrace -H(\theta,r) \rbrace,
\end{equation}
where $H(\theta,r)$ is the Hamiltonian system dynamics (HSD) equation with potential energy $-L(\theta)$ and kinetic energy $K(r)$, which represent the density functions of $\theta$ and $r$, respectively. The property of the dynamics is that it keeps the joint distribution invariant \parencite{Nishio2019Performance}. 

When new samples $(\theta^*,r^*)$ are drawn, while HSD is numerically approximated in discrete time space with the leapfrog method to maintain the total energy. The leapfrog methods requires two parameters: the step size $\epsilon$, how far the next draw is from current sample, and $L$, the number of steps to move in the process. When the process is finished, new samples are accepted with probability 
\begin{equation}
\alpha = \min \lbrace 1, \frac{f(\theta^*,r^*)}{ f(\theta,r)} \rbrace. 
\end{equation}

Due to the HMD high sensitivity to $\epsilon$ and $L$ where the choice may badly affect the results in HMC, \textcite{Hoffman2014NoUturn} proposed the No-U-Turn Sampler (NUTS), which determines the step size by adapting it during the warm-up (burn-in) phase to a target acceptance rate, and then use it for all sampling iterations \parencite{Monnahan2017Faster}. It also omits of $L$ by using the criterion 
\begin{eqnarray}
\frac{d}{dt}\frac{(\theta^*-\theta)\cdot(\theta^*-\theta)}{2}=(\theta^*-\theta)\cdot\frac{d}{dt}(\theta^*-\theta)=(\theta^*-\theta)\cdot r^* <0,
\end{eqnarray}
where $r^*$ is the current momentum and $(\theta^*-\theta)$ is the distance from initial position to current position. The idea is that the trajectory will keep exploring the space until $\theta^*$ starts to move back towards to $\theta$. 


To guarantee time reversibility and to converge to the correct distribution, NUTS overcomes this issue by means of a recursive algorithm that preserves reversibility by running the Hamiltonian simulation both forward and backward in time \parencite{Hoffman2014NoUturn}. This process starts with a slice variable $u$ which is uniformly distributed as $p(u\mid\theta, r) = U(0,f(\theta,r) )$, and generates a finite set of all $(\theta,r)$ in the doubling procedure and building binary trees process by randomly taking forward and backward leapfrog steps until 
\begin{equation}
\begin{matrix}
(\theta^+-\theta^-)\cdot r^- <0 & \mbox{or} & (\theta^+-\theta^-)\cdot r^+ <0,
\end{matrix}
\end{equation}
where $\theta^-, r^-$ and $\theta^+, r^+$ are the leftmost and rightmost leaves of $\theta$ and $r$ in the subtree. The best candidate $(\theta^*,r^*)$ is uniformly sampled form the subset of all candidate $(\theta,r)$. 

% Precise definition and description of NUTS algorithm can be found at \textcite{Hoffman2014NoUturn}. 



\section{Post-sampling checking}\label{sec:mcmcchain}

%\subsection{Markov chain diagnoses}
%
%Diagnose MCMC chains is essential to monitor whether the chains have converged in the process of posterior sampling. Traditionally, trace plots and auto-correlation functions were applied to diagnose Markov chains. Other than the mentioned methods, \textcite{gabry2019Visualization} suggest to use bi-variate scatter plots that mark the divergent transitions and divergence check plots, particularly, for HMC algorithm. 


\textcite{gelman2003Bayesian} suggests a few strategies for Bayesian model checks: (1) checking that the posterior inferences are reasonable, given the substantive context of the model; (2) examining the sensitivity of the inferences to reasonable changes in the prior distribution and the likelihood; and (3) checking that the model is capable of generating data like the observed data. One may refer to \textcite{gelman2004Exploratory, weiss2016Pediatric, Gelman2013Bayesian, Congdon2019Bayesian} for explanations and applications. We focus on the third strategy by graphically checking the replicate and observed data for the reproducible capability of our model. 


In ideal situation, researchers are able to use independent data set which is not used in modelling process to test the performances of different models. Alternatively, one may split one data set into training and testing data sets, and use the former one for model training and the latter one for testing. However, it is not feasible for some experimental agricultural data sets. Nevertheless, we can still check the model by using the discussed La Rossa data. 


\subsection{Posterior predictive checking}


The posterior predictive checking uses the posterior of the parameter in the model to regenerate the observations. The idea behind the concept is that if a model is a good fit we should be able to use it to generate data that resemble the data that we observed \parencite{gabry2019Visualization}. In other words, let $\bm{Y}^{rep}$ denotes the replicate data if the process that generated the data $\bm{Y}$ is replicated with the same value of $\theta$ that generated the observed data. Then $\bm{Y}^{rep}$ is governed by the posterior predictive distribution 
\begin{equation}
p(\bm{Y}^{rep} \mid \bm{Y}) = \int p(\bm{Y}^{rep} \mid \theta)p(\theta \mid \bm{Y}) d\theta. 
\end{equation}
The samples $\bm{Y}^{rep}$ is checked against the data $\bm{Y}$ \parencite{dipakdey2005Bayesian, Congdon2019Bayesian}. 


The application of posterior predictive distributions is more robust than prior specification, because the details of the prior are washed out by the likelihood \parencite{gelman2017Prior} if the model is good enough.



\subsection{Model diagnosis and evaluation}


Leave-one-out (LOO) cross validation (CV), where one data is omitted at one time and the fitted model based on the remaining data is the best predictor in terms of mean square error, is widely used for model evaluation and selection. In Bayesian statistics, to measure the predictive accuracy, researchers use the expected $\log$ LOO predictive density ELPD: 
\begin{equation}\label{eq:elpd}
\mbox{\elpd} = \sum_{i=1}^{n}\log p(y_i\mid y_{-i}),
\end{equation}
where $p(y_i \mid y_{-i}) = \int p(y_i \mid \theta)p(\theta \mid y_{-i})d\theta$ is the LOO predictive density with the $i$-th data omitted from the data set \parencite{Vehtari2017Practical}. However, it is too expensive in computing because it requires refitting the model $n$ times. \textcite{Burkner2020Efficient} proposed an approximated LOO CV using only a single model fit by instead calculating the pointwise $\log$ predictive density as a fast approximation to the exact LOO CV. It uses the Pareto-smoothed importance-sampling algorithm \parencite{Vehtari2017Practical} on the pointwise $\log$-likelihood matrix for each draw $m$ from the full posterior distribution and obtains a PSIS-LOO-CV estimate, which is
\begin{equation}
\widehat{\mbox{\psis}} = \sum_{i=1}^n\log\left(  \frac{ \sum_{m=1}^{M} p(y_i\mid \theta^{(m)})w_i^{(m)} }{ \sum_{m=1}^{M}w_i^{(m)} }\right), 
\end{equation}
where $w_i^{(m)}$ are stabilised weights by PSIS, $m = 1, \ldots, M$.  



The resulting PSIS LOO CV approximations can be used for model diagnosis and comparison. The advantage of PSIS is that it automatically computes an empirical similarity between the full data predictive distribution to the LOO predictive distribution for each left out point \parencite{gabry2019Visualization}, and the estimated tail shape parameter $\hat{k}$ of the generalised Pareto distribution can be used for assessing the reliability and approximate convergence rate of the estimates. If $\hat{k}<0.5$ then the distribution of raw importance ratios has finite variance and the central limit theorem holds. But in practice the model is still robust for $\hat{k}$ values up to 0.7. Otherwise the variance and the mean of the raw ratios distribution do not exist \parencite{Vehtari2017Practical}. 



%\textcite{Vehtari2017Practical} proposed Pareto-smoothed importance sampling (PSIS), which applies a smoothing procedure to the importance weights by fitting a generalised Pareto distribution to the upper tail, to estimate LOO CV. The PSIS estimate of the LOO expected $\log$ pointwise predictive density is 
%\begin{equation}
%\widehat{\mbox{elpd}}_{\mbox{psis-loo}}=\sum_{i=1}^{n}\log  \frac{\sum_{j=1}^{M}\omega_i^j p(y_i\mid \theta^j)}{\sum_{j=1}^{M}\omega_i^j},
%\end{equation}
%where $\omega_i^j$ is the truncated weights in the $j$-th posterior for the $i$-th data.



The Bayesian $R^2$, proposed by \textcite{Gelman2019Rsquared}, is used for model evaluation as well. $R^2$ is presented as the variance of the predicted values divided by the variance of predicted values plus the expected error variance 
\begin{eqnarray}
\mbox{Bayesian } R^2 = \frac{\Var(\bm{Y}^{pred})}{\Var (\bm{Y}^{pred})+\Var(\bm{e})}.
\end{eqnarray}
However, it should not be interpreted solely if the model has a large amount of bad Pareto $\hat{k}$ values i.e. values greater than 0.7 or, even worse, than 1. 



\section{Analysis of Las Rosas data}\label{sec:analysis}


\subsection{Data visualisation}

A part of Las Rosas data set, which is publicly available by the name of \texttt{lasrosas.corn} in the \R-package \texttt{agridat} \parencite{Edmondson2014Agridat}, was used in our study. The data were produced by a yield monitor in an Argentinian corn field trial conducted by incorporating six nitrogen rate treatments in three replicated blocks comprising 18 strips. In order to account for some of the spatial variation (Figure \ref{fig:lasrossa}), a four level topographic factor was defined: W (West slope), HT (Hilltop), E (East slope) and LO (Low East).  

%\begin{figure}[!htp]
%	\centering
%	\includegraphics[width=0.4\textwidth,height=6.3cm]{../Plots/lasrossa_view01}
%	\includegraphics[width=0.4\textwidth,height=6cm]{../Plots/lasrossa_view02}
%	\caption{Visualisation of Las Rosas data set.}\label{fig:lasrossa}
%\end{figure}
%\begin{figure}[!htp]
%	\centering
%	\includegraphics[width=0.4\textwidth]{../Plots/view_ridgehist}
%	\includegraphics[width=0.4\textwidth]{../Plots/view_densityhist}
%	\caption{Histogram and density plot of Las Rosas data set}\label{fig:lasrossadensity}
%\end{figure}


\begin{figure}[!htp]
	\centering	
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=6.3cm,width=\linewidth]{Images/lasrossa_view01}
		\caption{Visualisation of yield. Yellow colour indicates low yield and dark green indicates high yield.}
	\end{subfigure} 
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/lasrossa_view02}
		\caption{Coloured yield by topographic factors.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		%\includegraphics[height=6.3cm]{Images/lasrossa_view03}
		%\caption{Histogram of yield by different topographic factors: West slope (W), Hilltop (HT), East slope (E) and Low East (LO).}
		\includegraphics[height=6.3cm,width=\linewidth]{Images/lasrossa_view05}
		\caption{Linear models fitted within each topographic factor: West slope (W), Hilltop (HT), East slope (E) and Low East (LO).}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[height=5.9cm,width=\linewidth]{Images/lasrossa_view04}
		\caption{Bimodal histogram and density plot of yield.}
	\end{subfigure}
	\caption{Visualisation of Las Rosas yield monitor data for harvests in 2001.}\label{fig:lasrossa}
\end{figure}


In order to analyse the data set, we apply a geographic projection which transforms the geo-spatial coordinates to planar coordinates expressed in meters. The Las Rosas experiment is approximately 810m long and 150m wide \parencite{Rakshit2020Novel}.



\subsection{Statistical models and prior predictive simulations}


To demonstrate the power of the proposed model \eqref{eq:underlying} where the random parameters $\bm{u}$ are spatially correlated, we compare the model with the one without spatial correlation, used as a benchmark model. We also compare two distribution assumptions, the Gaussian  with the Student-$t$ distribution in order to decide which better specifies the model. Therefore, we define four models, as illustrated in Table \ref{tb:models}. 


\begin{table}[!htp]
	\centering
	\begin{tabular}{*{5}{l}} \toprule
		& Model 1 & Model 2& Model 3& Model 4  \\ \midrule
		Spatial correlation & No & Yes & No & Yes \\ 
		$\Var(\bm{u})$ &  $I_{n\times n}\otimes V_u$ & $V_s\otimes V_u$ & $I_{n\times n}\otimes V_u$ & $V_s\otimes V_u$ \\ 
		Distribution & Gaussian & Gaussian & Student & Student \\
		\bottomrule
	\end{tabular}\caption{Four models that are fitted in our study.}\label{tb:models}
\end{table}


The modelling process starts by choosing weakly informative priors for model 1. We can generate any data set that is plausibly observed through the model by using weakly informative prior. 

For instance vague priors are $b_0\sim N(\mu,100)$, $b_1,b_2\sim N(0,100)$ and $\sigma_e\sim IG(1,100)$, where $IG$ is the inverse Gamma distribution. Similarly, we assume $u_{ki}\sim N(0,\sigma_{k}^2)$ with $\sigma_k^2\sim IG(1,100)$ and $k=0, 1, 2$ at grid $s_i$. $\mu$ is either mean or median of the observed data. Alternatively, we can choose weakly informative priors $b_0\sim N(80,10)$, $b_1\sim N(0,0.01)$, $b_2\sim N(0,0.001)$, $\sigma_{0}\sim N_+(0,1)$, $\sigma_{1}\sim N_+(0,0.01)$, $\sigma_{2}\sim N_+(0,0.001)$, $R_u\sim \mbox{LKJcorr}(1)$ and $\sigma_e\sim N_+(0,1)$, where $N_+(\cdot)$ is the positive half Gaussian distribution. 


Figure \ref{fig:priorcheck} compares the regenerated data with vague and weakly informative priors. When the vague priors are applied, model 1 regenerates extremely small and large values which are implausible for yield data. This is mostly because the vague priors disregard practical knowledge. Applying weakly informative priors avoids negative values and keeps the simulations within reasonable interval. Even though some simulations are not perfect, the priors are overall good enough in respect to common knowledge. If the priors, on the other hand, are too informative, they may badly influence the posterior distribution and result in partial exploration of the posterior space. 

%where $t$ is the Student-$t$ distribution in the form
%\begin{equation}
%t(y\mid\nu,\mu,\tau) = \frac{\Gamma((\nu+1)/2)}{\Gamma(\nu/2)}\frac{1}{\sqrt{\nu\pi}\tau} \left(1+\frac{1}{\nu}  \left(\frac{y-\mu}{\tau} \right)^2  \right)^{-(\nu+1)/2}
%\end{equation}
%for degrees of freedom $\nu$ and scale parameter $\tau\in\mathcal{R}^+$, mean $\mu$ and $y\in\mathcal{R}$, and $t(\cdot)^+$ indicate a positive half-$t$ distribution with given parameters. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Images/priorcheck_vague}
		\caption{With vague priors}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\centering
		\includegraphics[width=\linewidth]{Images/priorcheck_weak}
		\caption{With weakly informative priors}
	\end{subfigure}
	\caption{Capability of re-generating observed data with different priors by running 100 simulations. Vague priors failed in regenerating and leads to extreme values. Weakly informative priors give plausible regenerated data. }\label{fig:priorcheck}
\end{figure}



In model 2, additionally to the priors used in model 1, we use priors: $\rho_c,\rho_r\sim U(0,1)$, a uniform distribution between 0 and 1. In model 3 and 4, the prior was assigned to the extra parameter of degrees of freedom $\nu\sim \mbox{Gamma}(2,0.1)$, as suggested by \textcite{Juarez2010ModelBased}, aiming to keep the prior simple. 

A summary of the priors is listed in Table \ref{tb:priors}. It is not recommended to use the same priors for different models. In addition, if a new prior is proposed for a new parameter, the prior predictive checking is recommended for each model. In this study, since our models regenerated plausible outcomes, we built up the models by using the same weakly informative priors and only adding priors for the new parameters, without changing the rest of the priors.


Additionally, NUTS does not require conjugate priors, which means the results are still valid if our priors are Gaussian but the data is Student distribution. 


\begin{table}[!htp]
	\centering
	\begin{tabular}{l *{4}{c}} \toprule
		& Model 1 & Model 2& Model 3& Model 4  \\ \midrule
		$b_0$ & \multicolumn{4}{c}{$N(80, 10)$} \\ 
		$b_1$ & \multicolumn{4}{c}{$N(0, 0.01)$} \\ 
		$b_2$ & \multicolumn{4}{c}{$N(0, 0.001)$} \\ 
		$\sigma_0$ & \multicolumn{4}{c}{$N_+(0, 1)$} \\ 
		$\sigma_1$ & \multicolumn{4}{c}{$N_+(0, 0.01)$} \\
		$\sigma_2$ & \multicolumn{4}{c}{$N_+(0, 0.001)$} \\ 
		$\sigma_e$ & \multicolumn{4}{c}{$N_+(0, 1)$} \\ \midrule
		$R_u$ & --- & LKJcorr(1) & --- & LKJcorr(1) \\ 
		$\rho_c$ & --- & $U(0,1)$ & --- & $U(0,1)$ \\ 
		$\rho_r$ & --- & $U(0,1)$ & --- &  $U(0,1)$ \\ 		
		$\nu$ & --- &  --- & $Gamma(2,0.1)$ & $Gamma(2,0.1)$ \\ 
		\bottomrule
	\end{tabular}\caption{Priors of four models.}\label{tb:priors}
\end{table}

With the above priors, the models were run on four parallel Markov chains in \rstan\ with each chain containing 2000 iterations, where the first 1000 iterations are run in the warm-up phase. The final posterior contains 4000 samples for each parameter. 




\subsection{Posterior checking}


The prior predictive checking is a powerful tool for understanding the structure of the model. However, it is not possible to extend this technique to model selection and performance evaluation. We use MCMC diagnostic tools and posterior predictive checking for further analyses and comparison. 


First we use posterior predictive checking to visualise the performance of the four models. Figure \ref{fig:ppcheck} displays the results of the PP checking. If the random parameters are not spatially correlated, model 1 and 3 are incapable in regenerating the data and capturing adequately  the distribution of the observations. On contrary, models 2 and 4 show promising results. 


\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_GSRNS}
		\caption{PP check of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_GSRand}
		\caption{PP check of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_STRNS}
		\caption{PP check of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/ppcheck_STRand}
		\caption{PP check of Model 4}
	\end{subfigure}
	\caption{Posterior predictive checking for simple linear and the proposed spatial models with 100 simulations.}\label{fig:ppcheck}
\end{figure}


Figure \ref{fig:skewcheck} illustrates the observed skewness of the posterior predictive distribution for the four models. While models 2 and 4 capture the observed skewness, model 1 and 3 demonstrate model mis-specification. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_GSRNS}
		\caption{Skew check of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_GSRand}
		\caption{Skew check of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_STRNS}
		\caption{Skew check of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/skew_STRand}
		\caption{Skew check of Model 4}
	\end{subfigure}
	\caption{Histograms of skewness for 4000 draws from the posterior predictive distribution.}\label{fig:skewcheck}
\end{figure}


To visualise the performance of the models, we use the LOO CV predictive cumulative density plots, which are asymptotically uniform (for continuous data) if the model is well calibrated \parencite{gabry2019Visualization, Gelman2013Bayesian}. Figure \ref{fig:pitloo} compares the density of the computed LOO PIT (the thick dark curve) versus 100 simulated data sets from a standard uniform distribution (the thin light curves). Model 1 and 3 are obviously mis-calibrated. The frown shape by model 2 indicates that the model is overall good but still a little mis-calibrated comparing to Model 4. This indicates that the model is either mis-specified or too flexible. Model 4 performs the best among all four models. 

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_GSRNS}
		\caption{LOO PIT diagnostic of Model 1}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_GSRand}
		\caption{LOO PIT diagnostic of Model 2}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_STRNS}
		\caption{LOO PIT diagnostic of Model 3}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/pit_STRand}
		\caption{LOO PIT diagnostic of Model 4}
	\end{subfigure}
	\caption{LOO PIT plots of the four models. }\label{fig:pitloo}
\end{figure}


Pareto $\hat{k}$ diagnosis is vital in Bayesian analysis as well. Model 1 has too many large $\hat{k}$ values, which indicates that the model is either misspecified or too flexible. Similarly for model 3 that there are 24 ``bad'' values. These results should not be interpreted solely but together with LOO PIT and \ploo\ values in Table \ref{tb:LOOR}. If we look at Figure \ref{fig:ppcheck}, it is clearly that model 1 and model 3 are misspecified. The LOO PIT plots (Figure \ref{fig:pitloo}) confirm it even if there are no ``bad'' $\hat{k}$ values. In case where high Pareto $\hat{k}$ values are observed but the model fit is good, the model is both misspecified and flexible. The latter indicates that the model has a good capability in predicting unknown data. In this scenario, $K$-fold CV is recommended. 


For model 2, there is one high $\hat{k}$ value, which could be a high influential point or an outlier. However, it still indicates that there might be an issue with the model. Therefore, instead of using Gaussian distribution, model 4 uses Student-$t$ distribution and all $\hat{k}$ values are small (less than 0.7). Then LOO CV does the balancing of goodness-of-fit and parsimony automatically, and \elpd\ and Bayesian $R^2$ are valid. 

\begin{table}[!htp]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{ l *{12}{c} } \toprule 
			& \multicolumn{3}{c}{Model 1}   & \multicolumn{3}{c}{Model 2} & \multicolumn{3}{c}{Model 3}   & \multicolumn{3}{c}{Model 4} \\
			&  Count & Per & M.Eff  &  Count & Per & M.Eff  &  Count & Per & M.Eff  &  Count & Per & M.Eff   \\ \midrule
			(-Inf, 0.5] (good)  &  28    &  1.7\%   & 457 &  1673 & 99.9\% & 831&1474 & 88.1\%& 494 & 1673 & 99.9\% & 2990 \\   
			(0.5, 0.7] (ok)      &  372  &  22.2\% & 112 &     0  & 0\%        & ---  & 176  & 10.5\% & 254 &   1  & 0.1\% & 589  \\
			(0.7, 1] (bad)       &  1138&  68.0\% & 18   &     0 &  0\%        & ---  &  24   &  1.4\%  & 170 &  0    &  0.0\%& ---  \\
			(1, Inf) (very bad)&  136  &  8.1\%   & 8     &     1  & 0.1\%      & 11 &  0    &     0\%   & ---   &  0   & 0\%     & ---  \\
			\bottomrule
	\end{tabular}}
	\caption{Pareto $\hat{k}$ diagnostic values including count, percentage (Per) and minimal effective sample sizes (M.Eff) for all models.}\label{tb:Pareto}
\end{table}



\subsection{Model evaluation}


We use ELPD, LOO CV sores and Bayesian $R^2$ to evaluate and compare the performance of different models. \ploo\ is the effective number of parameters and is calculated as the subtraction of \elpd\ and the non-cross-validated $\log$ posterior predictive density. In Bayesian analysis, even if there are no high Pareto $\hat{k}$ values, $R^2$ is not indicative if \ploo\ is relatively high compared to the total number of parameters or the number of observations, which implies weakly predictive capability and probably a model misspecification. 


The results form our study are shown in Table \ref{tb:LOOR}. Other than mean and standard deviation of the posterior distribution, the credibility interval (CI) is reported in the table as well. The level $\alpha$ equal tail credibility interval is the interval bracketed by the $\alpha/2$ and the $1-\alpha/2$ quantiles of the posterior samples, where $\alpha \in (0,1)$. We chose $\alpha=0.05$, a standard option of the frequentists. 


\begin{table}[!htp]
	\centering
	\resizebox{\textwidth}{!}{\begin{tabular}{ l *{8}{c} } \toprule 
			& \multicolumn{2}{c}{Model 1}   & \multicolumn{2}{c}{Model 2} & \multicolumn{2}{c}{Model 3}   & \multicolumn{2}{c}{Model 4} \\
			\midrule
			&  Estimate & SE &  Estimate & SE &  Estimate & SE &  Estimate & SE    \\ 
			\elpd &  -7236.2  & 13.4 & -5253.4 &100.6 &-7848.4 & 17.1 & -5133.0& 37.8 \\   
			\ploo  & 1487.1     & 11.7 & 91.9  &16.6  &241.2  & 6.8 & 97.9 &  2.4 \\
			looic     &  14472.5  & 26.7 & 10506.7 & 201.1 &  15696.8 & 34.3 & 10266.0 & 75.7\\		
			\midrule
			&  Median & CI &   Median & CI &   Median & CI &   Median & CI   \\ 
			Bayesian $R^2$   &  0.842   & 0.563$\sim$0.965 &  0.956  & 0.953$\sim$0.959 & 0.190 & 0.135$\sim$0.251 & 0.974 & 0.970$\sim$0.977 \\
			\bottomrule
	\end{tabular}}
	\caption{LOO CV estimates with standard errors and medians of Bayesian $R^2$ credibility intervals.}\label{tb:LOOR}
\end{table}


Even if model 1 has higher $R^2$ values compared to model 3, because of the few high Pareto $\hat{k}$ values and large \ploo\ values, the $R^2$ should not be used in the interpretations of the results. The focus should be on models 2 and 4. Apparently, Student-$t$ is better than Gaussian distribution in terms of smaller LOO CV score and higher $R^2$ value. 




\section{Results}

\subsection{Model assessment}
Table \ref{tb:resultModel4} presents summary statistics of the posterior distribution of all parameters.
It should be noted that the means and the medians for all parameters are very close or identical which indicates very robust results. Another feature is the magnitude of the values of $\hat{b}_2$ and $\hat{\sigma}_2$, they are very small. The latter indicates a week influence of the quadratic term of the regression. The pattern of coefficients magnitude is well illustrated on Figure \ref{fig:betasprd}.

\begin{table}[!htp]\centering
	\begin{tabular}{ l *{5}{c}} \toprule
		\multirow{2}{*}[-2pt]{Parameter}  &  \multirow{2}{*}[-2pt]{Mean}   & \multirow{2}{*}[-2pt]{SD}  &   \multicolumn{3}{c}{Credibility interval}  \\  \cmidrule{4-6} 
		&     &    &    2.5\%   &       Median  &      97.5\% \\ \midrule 
		$\hat{b}_0$   &   88.700  & 2.834 & 83.473 & 88.549 & 94.627   \\
		$\hat{b}_1$   &  0.031 & 0.009 & 0.013 & 0.031& 0.047 \\
		$\hat{b}_2(\times 10^{-4})$   & 1.140 & 0.690 & -0.180 &  1.131 & 2.545  \\
		$\hat{\sigma}_0$&     2.453 & 0.564 & 1.487 &  2.412 &  3.670   \\
		$\hat{\sigma}_1$&   0.011 &  0.005 & 0.004 &  0.010 &  0.022   \\
		$\hat{\sigma}_2(\times 10^{-4})$&   3.876 &  3.785 &  0.415 &  2.510 & 14.370    \\		
		$\hat{\sigma}_e$& 4.113 & 0.136 & 3.853 & 4.113 & 4.386   \\
		$\hat{\rho}_{12}$& -0.464 & 0.313 & -0.931 & -0.504 &  0.217 \\
		$\hat{\rho}_{13}$& 0.210 &  0.359 & -0.500 &  0.221 &  0.863  \\
		$\hat{\rho}_{23}$& -0.135 & 0.458 & -0.897 & -0.154 &  0.721  \\		
		$\hat{\rho}_r$   &  0.977  & 0.008  & 0.960  & 0.977  & 0.990   \\
		$\hat{\rho}_c$     &   0.990  & 0.007  & 0.973  & 0.992  & 0.998  \\
              		$\hat{\nu}$ &  5.40  &  0.783  & 4.122  & 5.318  & 7.140  \\
		\bottomrule
	\end{tabular}\caption{Summary statistics of the posterior samples from model 4.}\label{tb:resultModel4}
\end{table}         

Figure \ref{fig:betasprd} displays the overall coefficients of the fixed and random components on three separate plots, for the intercept $\hat{\bm{\beta}}_0 = \hat{\bm{b}}_0+\tilde{\bm{u}}_0$, the linear term $\hat{\bm{\beta}}_1 = \hat{\bm{b}}_1+\tilde{\bm{u}}_1$ and the quadratic term $\hat{\bm{\beta}}_2 = \hat{\bm{b}}_2+\tilde{\bm{u}}_2$, respectively. The plots cover the whole trial area, as presented on Figure \ref{fig:lasrossa}(a) and \ref{fig:lasrossa}(b). 
The contour maps are aligned with the topology of the area. It can be observed that the Hilltop area and small part of the neighbouring areas on the left and right (see Figure \ref{fig:lasrossa}(b)) are exhibiting different pattern in comparison to the other three topological regions, for all of the $\hat{\bm{\beta}}$ coefficients. 

Furthermore, seems that for the Hilltop area the linear component coefficient is the highest, in the range of 0.14-0.18, while for the other three areas is around 0.02. The latter indicates

The contour maps reflect the  In the Hilltop (middle area) and eastern of West slope (left side) areas, the quadratic term $\bm{\beta}_2$ is negative, which means the optimal treatment level exists. 

\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b0}
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b1}
	\includegraphics[width=\textwidth, height=6cm]{Images/STRand_b2}
	\caption{Contour plots of overall $\hat{\bm{\beta}}_0$ (top), $\hat{\bm{\beta}}_1$ (middle) and $\hat{\bm{\beta}}_2$ (bottom). }\label{fig:betasprd}
\end{figure}






\subsection{Yield prediction}

Based on the results from the previous section, we are now able to predict yield for any given nitrogen rates. Suppose we apply the medium nitrogen rate 75.4 kg/ha on the entire farm, the yield is calculated by substituting it into the model, where parameters are drawn from the posterior distribution. After a few iterations, we may calculate the median of yield for each grid. The result is shown in Figure \ref{fig:yieldprd}. 


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/STRand_prd}
	\caption{Predicted yield with medium nitrogen rate 75.4 kg/ha across the field.}\label{fig:yieldprd}
\end{figure}



\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/ST_opNitrogen}
	\includegraphics[width=\textwidth]{Images/ST_opYield}
	\caption{Optimal nitrogen rates (top) and estimated yield with the rates (bottom).}\label{fig:optN}
\end{figure}


\begin{figure}[!htp]
	\centering	
	\includegraphics[width=\textwidth]{Images/ST_DiffYield}
	\caption{Yield difference of predicted values with optimal nitrogen and with medium nitrogen level.}\label{fig:stdiff}
\end{figure}



\subsection{Comparing to GWR}


GWR approach, proposed by \textcite{Rakshit2020Novel}, uses geographic weighted local regression to estimate the optimal nitrogen rate for each plot and to predict the yield for OFE. Compared to GWR, the Bayesian approach uses Bayes theorem and NUTS sampler to explore the posterior distribution of the objective and, with Bayesian inference, to estimate the parameters for each plot. 


They are two different paths but aiming at the same goal: providing easy interpretation and visual contour plots for plant growers. Each of these approaches has its own advantages. 


For GWR, the reliability of the results relies on the bandwidth of the moving window. The optimal bandwidth is selected by cross validation. All data points within the window are used for inferring the information of the query plot. Other data points, which are out of the window, contributes nothing to the target plot. It has ``higher resolution'' that the optimal treatments at pairwise plots are distinguishing, even though the adjusted-$p$ values are large. On the contrary, the proposed model with Bayesian approach uses all data with the spatial variance matrix on the entire field to estimate the query plot. The nearer plots contributes more and further plots contributes less in the inference. So it has ``lower resolution'', which can be seen from Figure \ref{fig:optN}, that the the difference of optimal nitrogen rates in West and East ares are not significant. Additionally, the reliability of the Bayesian approach is affected by the priors and the model itself, whereas the influence of the priors is ``washed out'' if the model is good enough. Additionally, GWR is an ad hoc approach for addressing a particular question. The Bayesian approach has more flexibility to extend and to broadly applied to other questions. 


A comparison of these two approaches is summarised in Table \ref{tb:compareGWR}. 

\begin{table}[!htp]
	\centering
	\begin{tabular}{*{3}{l}} \toprule
		& GWR & Bayesian \\ \midrule
		Inference	& with neighbouring data & with all data \\ 	 
		Initialisation	& bandwidth selection &  prior specification \\ 	
		Objective	& local log-likelihood & global log-likelihood \\ 
		Distinguishability & High & Low \\ \midrule
		Evaluation	&  $t$ scores and $p$-values & credible intervals \\
		&  & PP check and LOO PIT \\
		&  & Pareto $k$ diagnostic \\
		&  & Bayesian $R^2$ \\ 
		\bottomrule
	\end{tabular}
	\caption{Comparison of GWR and Bayesian approach. Each of these two approach has its own advantages and disadvantages.}\label{tb:compareGWR}
\end{table}


\section{Conclusion}


This paper demonstrate the Bayesian framework to analyse spatial varying on farm trial experiment. Starting from the prior selection, we explain the mechanism of Bayesian approach and NUTS sampler. We also present the model checking and diagnostic process for post-sampling. Compared with GWR, Bayesian approach does not require bandwidth selection, but requires pre-specified priors. The results from Bayesian approach are similar with GWR and is able to capture more detailed information on the field. We open our \rstan\ function to public, with which one my apply to their own data sets. 



\section*{Acknowledgement}

SAGI West gratefully acknowledges the support from the Grains Research and Development Corporation of Australia. The computation in this paper has been performed using the \R-packages \rstan. 



\appendix
\section*{Appendix}

\subsection*{Prior predictive checking}

\begin{figure}[!htp]
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_GSRNS}
		\caption{Model 1: Gaussian distribution without spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_GSRand}
		\caption{Model 2: Gaussian distribution with spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\linewidth]{Images/prior_STRNS}
		\caption{Model 3: Student distribution without spatial correlation.}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}		
		\includegraphics[width=\linewidth]{Images/prior_STRand}
		\caption{Model 4: Student distribution with spatial correlation.}
	\end{subfigure}
	\caption{Weakly informative priors checking on four models. }\label{fig:priorcheck4models}
\end{figure}



\subsection*{Faster Cholesky factor for $AR1(\rho)$}\label{sec:fastar1}

The $AR1(\rho)$ correlation matrix with correlation factor $\rho$ is defined by $\rho_{ij} = \rho^{|i-j|}$. Not well-known but real helpful that a simple form of Cholesky factor for the $AR1(\rho)$ structure is given by \textcite{Madar2015Direct} that the simple form is 
\begin{equation}
l_{ij} = \begin{cases}
\rho^{j-1} & j\geq i =1 \\
\rho^{j-i} \sqrt{1-\rho^2} & j\geq i \geq2
\end{cases}
\end{equation}
which significantly improve the computation efficiency in \rstan. 

\subsection*{Faster Kronecker product}\label{sec:kronec}

Suppose $A=L_A L_A^\top$ and $B=L_B L_B^\top$ are respectively $N\times N$ and $M\times M$ matrices with Cholesky decomposition $L_A$ and $L_B$, Kronecker product has a good property that 
\begin{equation}
C=A\otimes B = (L_A L_A^\top)\otimes (L_B L_B^\top) = (L_A\otimes L_B)(L_A \otimes L_B)^\top,
\end{equation}
where the new matrix $C$ is $NM\times NM$, and the elements of the new matrix $c_{p,q}=a_{i,j}b_{kl}$, where $p=M(i-1)+k$ and $q=M(j-1)+l$. Similarly, the Kronecker product of three matrices, with a $K\times K$ matrix $C$, will be
\begin{equation}
D=A\otimes B\otimes C = (L_A\otimes L_B\otimes L_C) (L_A\otimes  L_B\otimes L_C)^\top,
\end{equation}
and the new elements are $d_{p,q}=a_{i,j}b_{k,l}c_{m,n}$, where $p=K(M(i-1)+k-1)+m$ and $q=K(M(j-1)+l-1)+n$. 

Additionally, with the above properties, we use the following formula to increase the computation efficiency, that
\begin{equation}
(L_A\otimes L_B\otimes L_C)(Z_1\otimes Z_2\otimes Z_3) = (L_AZ_1)\otimes (L_BZ_2)\otimes (L_C Z_3),
\end{equation}
where $Z_1$, $Z_2$ and $Z_3$ are vectors of length $N$, $M$ and $K$ respectively. This formula is used in \rstan\ and considerably saves computation time. For interests of other properties of Kronecker product, one can refer to \textcite{Zhang2013Kronecker}.



\renewcommand\bibname{References}% change bibliography title to references
%\addcontentsline{toc}{chapter}{Bibliography}
\addtocontents{toc}{Bibliography}
\printbibliography
\end{document}
